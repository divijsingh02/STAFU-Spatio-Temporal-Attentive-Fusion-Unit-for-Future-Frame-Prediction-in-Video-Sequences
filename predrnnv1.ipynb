{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IMPORTS**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.utils.data as data\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport numpy as np\nfrom tqdm import tqdm\nfrom numpy import *\nfrom numpy.linalg import *\nfrom scipy.special import factorial\nfrom functools import reduce\nimport random\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport time\nimport gzip\nimport cv2\nimport math\nimport os\nfrom PIL import Image\nfrom skimage.metrics import structural_similarity as compare_ssim\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.transform import resize\nimport argparse\n!pip install lpips\nimport codecs\nimport lpips\n#!pip install pynvml\n#import pynvml\n\nimport torch\nfrom torch import nn\nfrom torch import  optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset\nimport torchvision as tv\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\n\nfrom PIL import Image\nimport numpy as np\nimport os\nimport math\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport cv2\n\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.transform import resize\nimport argparse\n!pip install lpips\nimport codecs\nimport lpips\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport argparse\nimport os\nimport random\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport time\nfrom tqdm import trange\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n#from utils_3d import split, merge, psi\nimport torch.backends.cudnn as cudnn\nimport functools\nfrom torch.nn import init\nimport random","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:08.765596Z","iopub.execute_input":"2022-06-19T17:15:08.766419Z","iopub.status.idle":"2022-06-19T17:15:27.987718Z","shell.execute_reply.started":"2022-06-19T17:15:08.766369Z","shell.execute_reply":"2022-06-19T17:15:27.986582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY**","metadata":{}},{"cell_type":"code","source":"def reshape_patch(img_tensor, patch_size):\n    assert 4 == img_tensor.ndim\n    seq_length = np.shape(img_tensor)[0]\n    img_height = np.shape(img_tensor)[1]\n    img_width = np.shape(img_tensor)[2]\n    num_channels = np.shape(img_tensor)[3]\n    a = np.reshape(img_tensor, [seq_length,\n                                img_height // patch_size, patch_size,\n                                img_width // patch_size, patch_size,\n                                num_channels])\n    b = np.transpose(a, [0, 1, 3, 2, 4, 5])\n    patch_tensor = np.reshape(b, [seq_length,\n                                  img_height // patch_size,\n                                  img_width // patch_size,\n                                  patch_size * patch_size * num_channels])\n    return patch_tensor\n\n\ndef reshape_patch_back(patch_tensor, patch_size):\n    # B L H W C\n    assert 5 == patch_tensor.ndim\n    # patch_tensor = patch_tensor.transpose(0, 1, 3, 4, 5, 2)\n    batch_size = np.shape(patch_tensor)[0]\n    seq_length = np.shape(patch_tensor)[1]\n    patch_height = np.shape(patch_tensor)[2]\n    patch_width = np.shape(patch_tensor)[3]\n    channels = np.shape(patch_tensor)[4]\n    img_channels = channels // (patch_size * patch_size)\n    a = np.reshape(patch_tensor, [batch_size, seq_length,\n                                  patch_height, patch_width,\n                                  patch_size, patch_size,\n                                  img_channels])\n    b = np.transpose(a, [0, 1, 2, 4, 3, 5, 6])\n    img_tensor = np.reshape(b, [batch_size, seq_length,\n                                patch_height * patch_size,\n                                patch_width * patch_size,\n                                img_channels])\n    return img_tensor\n\n\ndef reshape_patch_back_tensor(patch_tensor, patch_size):\n    # B L H W C\n    assert 5 == patch_tensor.ndim\n    # patch_tensor = patch_tensor.transpose(0, 1, 3, 4, 5, 2)\n    patch_narray = patch_tensor.detach().cpu().numpy()\n    batch_size = np.shape(patch_narray)[0]\n    seq_length = np.shape(patch_narray)[1]\n    patch_height = np.shape(patch_narray)[2]\n    patch_width = np.shape(patch_narray)[3]\n    channels = np.shape(patch_narray)[4]\n    img_channels = channels // (patch_size * patch_size)\n    a = torch.reshape(patch_tensor, [batch_size, seq_length,\n                                     patch_height, patch_width,\n                                     patch_size, patch_size,\n                                     img_channels])\n    b = a.permute([0, 1, 2, 4, 3, 5, 6])\n    img_tensor = torch.reshape(b, [batch_size, seq_length,\n                                   patch_height * patch_size,\n                                   patch_width * patch_size,\n                                   img_channels])\n    return img_tensor.permute(0, 1, 4, 2, 3)\n\n\ndef reshape_patch_tensor(img_tensor, patch_size):\n    assert 4 == img_tensor.ndim\n    seq_length = img_tensor.shape[0]\n    img_height = img_tensor.shape[1]\n    img_width = img_tensor.shape[2]\n    num_channels = img_tensor.shape[3]\n    a = torch.reshape(img_tensor, [seq_length,\n                                   img_height // patch_size, patch_size,\n                                   img_width // patch_size, patch_size,\n                                   num_channels])\n    b = a.permute((0, 1, 3, 2, 4, 5))\n    patch_tensor = torch.reshape(b, [seq_length,\n                                     img_height // patch_size,\n                                     img_width // patch_size,\n                                     patch_size * patch_size * num_channels])\n    return patch_tensor.permute((0, 3, 1, 2))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:27.991256Z","iopub.execute_input":"2022-06-19T17:15:27.991635Z","iopub.status.idle":"2022-06-19T17:15:28.015028Z","shell.execute_reply.started":"2022-06-19T17:15:27.991588Z","shell.execute_reply":"2022-06-19T17:15:28.013803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Norm(object):\n    def __init__(self, max=255):\n        self.max = max\n\n    def __call__(self, sample):\n        video_x = sample\n        new_video_x = video_x / self.max\n        return new_video_x\n\n\nclass ToTensor(object):\n\n    def __call__(self, sample):\n        video_x = sample\n        video_x = video_x.transpose((0, 3, 1, 2))\n        video_x = np.array(video_x)\n        return torch.from_numpy(video_x).float()\n    \n\nclass Resize(object):\n\n    def __call__(self, sample):\n        imgs_out = np.zeros((\n            sample.shape[0], configs.img_height, configs.img_width, sample.shape[3]))\n        for i in range(sample.shape[0]):\n            imgs_out[i,:,:,:] = resize(sample[i,:,:,:], imgs_out.shape[1:])\n        return imgs_out\n    \nclass Resize2(object):\n\n    def __call__(self, sample):\n        imgs_out = np.zeros((\n            sample.shape[0], 640, 480, sample.shape[3]))\n        for i in range(sample.shape[0]):\n            imgs_out[i,:,:,:] = resize(sample[i,:,:,:], imgs_out.shape[1:])\n        return imgs_out","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.016874Z","iopub.execute_input":"2022-06-19T17:15:28.01721Z","iopub.status.idle":"2022-06-19T17:15:28.034087Z","shell.execute_reply.started":"2022-06-19T17:15:28.017167Z","shell.execute_reply":"2022-06-19T17:15:28.033044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.03742Z","iopub.execute_input":"2022-06-19T17:15:28.038116Z","iopub.status.idle":"2022-06-19T17:15:28.046807Z","shell.execute_reply.started":"2022-06-19T17:15:28.03807Z","shell.execute_reply":"2022-06-19T17:15:28.045547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reserve_schedule_sampling_exp(itr):\n    if itr < args.r_sampling_step_1:\n        r_eta = 0.5\n    elif itr < args.r_sampling_step_2:\n        r_eta = 1.0 - 0.5 * math.exp(-float(itr - args.r_sampling_step_1) / args.r_exp_alpha)\n    else:\n        r_eta = 1.0\n\n    if itr < args.r_sampling_step_1:\n        eta = 0.5\n    elif itr < args.r_sampling_step_2:\n        eta = 0.5 - (0.5 / (args.r_sampling_step_2 - args.r_sampling_step_1)) * (itr - args.r_sampling_step_1)\n    else:\n        eta = 0.0\n\n    r_random_flip = np.random.random_sample(\n        (args.batch_size, args.input_length - 1))\n    r_true_token = (r_random_flip < r_eta)\n\n    random_flip = np.random.random_sample(\n        (args.batch_size, args.total_length - args.input_length - 1))\n    true_token = (random_flip < eta)\n\n    ones = np.ones((args.img_height // args.patch_size,\n                    args.img_width // args.patch_size,\n                    args.patch_size ** 2 * args.img_channel))\n    zeros = np.zeros((args.img_height // args.patch_size,\n                      args.img_width // args.patch_size,\n                      args.patch_size ** 2 * args.img_channel))\n\n    real_input_flag = []\n    for i in range(args.batch_size):\n        for j in range(args.total_length - 2):\n            if j < args.input_length - 1:\n                if r_true_token[i, j]:\n                    real_input_flag.append(ones)\n                else:\n                    real_input_flag.append(zeros)\n            else:\n                if true_token[i, j - (args.input_length - 1)]:\n                    real_input_flag.append(ones)\n                else:\n                    real_input_flag.append(zeros)\n\n    real_input_flag = np.array(real_input_flag)\n    real_input_flag = np.reshape(real_input_flag,\n                                 (args.batch_size,\n                                  args.total_length - 2,\n                                  args.img_height // args.patch_size,\n                                  args.img_width // args.patch_size,\n                                  args.patch_size ** 2 * args.img_channel))\n    return real_input_flag\n\n\ndef schedule_sampling(eta, itr):\n    zeros = np.zeros((args.batch_size,\n                      args.total_length - args.input_length - 1,\n                      args.img_height // args.patch_size,\n                      args.img_width // args.patch_size,\n                      args.patch_size ** 2 * args.img_channel))\n    if not args.scheduled_sampling:\n        return 0.0, zeros\n\n    if itr < args.sampling_stop_iter:\n        eta -= args.sampling_changing_rate\n    else:\n        eta = 0.0\n    random_flip = np.random.random_sample(\n        (args.batch_size, args.total_length - args.input_length - 1))\n    true_token = (random_flip < eta)\n    ones = np.ones((args.img_height // args.patch_size,\n                    args.img_width // args.patch_size,\n                    args.patch_size ** 2 * args.img_channel))\n    zeros = np.zeros((args.img_height // args.patch_size,\n                      args.img_width // args.patch_size,\n                      args.patch_size ** 2 * args.img_channel))\n    real_input_flag = []\n    for i in range(args.batch_size):\n        for j in range(args.total_length - args.input_length - 1):\n            if true_token[i, j]:\n                real_input_flag.append(ones)\n            else:\n                real_input_flag.append(zeros)\n    real_input_flag = np.array(real_input_flag)\n    real_input_flag = np.reshape(real_input_flag,\n                                 (args.batch_size,\n                                  args.total_length - args.input_length - 1,\n                                  args.img_height // args.patch_size,\n                                  args.img_width // args.patch_size,\n                                  args.patch_size ** 2 * args.img_channel))\n    return eta, real_input_flag","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.048772Z","iopub.execute_input":"2022-06-19T17:15:28.049555Z","iopub.status.idle":"2022-06-19T17:15:28.075664Z","shell.execute_reply.started":"2022-06-19T17:15:28.049445Z","shell.execute_reply":"2022-06-19T17:15:28.074517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DATA LOADER**","metadata":{}},{"cell_type":"code","source":"class TimeSeriesDatasetNPZ(data.Dataset):\n    def __init__(self, root_dir, n_frames_input=10, n_frames_output=10):\n        self.n_frames_in = n_frames_input\n        self.n_frames_out = n_frames_output\n        random.seed(420)\n        n_frames = n_frames_input + n_frames_output\n        \n        self.file = np.load(root_dir).transpose(1,0,2,3)[..., np.newaxis].transpose(0,1,4,2,3)#[:10]        \n            \n            \n    def __len__(self):\n        return len(self.file)\n\n    def __getitem__(self, index):\n        clips = torch.from_numpy(self.file[index])\n        clips = clips.type(torch.float32)\n        clips = (clips / 255)\n        return clips","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.079827Z","iopub.execute_input":"2022-06-19T17:15:28.08239Z","iopub.status.idle":"2022-06-19T17:15:28.092203Z","shell.execute_reply.started":"2022-06-19T17:15:28.082354Z","shell.execute_reply":"2022-06-19T17:15:28.091029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# td = TimeSeriesDataset(root_dir='../input/casiagdb/GaitDatasetB-silh', n_frames_input=10, n_frames_output=10)\n# train_loader = torch.utils.data.DataLoader(dataset=td, batch_size=1, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.096111Z","iopub.execute_input":"2022-06-19T17:15:28.09674Z","iopub.status.idle":"2022-06-19T17:15:28.105879Z","shell.execute_reply.started":"2022-06-19T17:15:28.096689Z","shell.execute_reply":"2022-06-19T17:15:28.104733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# z = next(iter(train_loader))\n# print(z.shape)\n# torch.max(z), torch.min(z)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.107827Z","iopub.execute_input":"2022-06-19T17:15:28.108604Z","iopub.status.idle":"2022-06-19T17:15:28.116224Z","shell.execute_reply.started":"2022-06-19T17:15:28.108556Z","shell.execute_reply":"2022-06-19T17:15:28.115081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesDatasetNpz(Dataset):\n    def __init__(self, root_dir, n_frames_input=10, n_frames_output=10):\n        self.n_frames_in = n_frames_input\n        self.n_frames_out = n_frames_output\n        n_frames = n_frames_input + n_frames_output\n        \n        self.file = np.load(root_dir).transpose(1,0,2,3)[..., np.newaxis].transpose(0,1,4,2,3)[:5000]          \n            \n    def __len__(self):\n        return len(self.file)\n\n    def __getitem__(self, index):\n        clips = torch.from_numpy(self.file[index])\n        clips = clips.type(torch.float32)\n        clips = (clips / 255)\n        return clips # (clips - 0.5) / 0.5  # Norm (-1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.119863Z","iopub.execute_input":"2022-06-19T17:15:28.120155Z","iopub.status.idle":"2022-06-19T17:15:28.129215Z","shell.execute_reply.started":"2022-06-19T17:15:28.120096Z","shell.execute_reply":"2022-06-19T17:15:28.127966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesDataset(data.Dataset):\n    def __init__(self, root_dir, n_frames_input=10, n_frames_output=10):\n        view_type='090'\n        random.seed(1000)\n        self.n_frames_in = n_frames_input\n        self.n_frames_out = n_frames_output\n        n_frames = n_frames_input + n_frames_output\n        subject_dirs = [d for d in os.listdir(root_dir)] # [01, 02, 03]\n        subject_dirs = [os.path.join(root_dir, subject_dirs[i]) for i in range(len(subject_dirs))] # root/01/01/\n        seq_type_dirs = [os.path.join(d, sd) for d in subject_dirs for sd in os.listdir(d)] # [root/01/01/bg1, root/01/01/bg2]\n        view_type_dirs = sorted([[os.path.join(d, sd)] for d in seq_type_dirs for sd in os.listdir(d) if sd == view_type])\n\n        self.specific_view_files = []\n        print(len(view_type_dirs))\n        \n        for d in view_type_dirs:\n            self.specific_view_files.append(sorted([os.path.join(d[0], f) for f in os.listdir(d[0])]))\n        \n        self.nframes_list = []\n        for f in self.specific_view_files[:50]:\n            for i in range(len(f)-n_frames):\n                self.nframes_list.append(f[i:i+n_frames])\n        \n        self.trans_norm = transforms.Compose([Norm()])\n        self.trans_tensor = transforms.Compose([ToTensor()])\n        self.trans_resize = transforms.Compose([Resize()])\n        \n        print(len(self.nframes_list))\n            \n            \n    def __len__(self):\n        return len(self.nframes_list)\n\n    def __getitem__(self, index):\n        data_seq = np.ndarray(shape=(len(self.nframes_list[index]), 64, 64), dtype=np.uint8)\n        \n        for i, f in enumerate(self.nframes_list[index]):\n            data_seq[i, :] = cv2.resize(np.array(Image.open(f)), (64, 64))\n            \n        data_seq = data_seq[..., np.newaxis]\n        input = self.trans_norm(data_seq)  \n        #input = self.trans_resize(input)\n        #input = reshape_patch(input, configs.patch_size)\n        input = self.trans_tensor(input)\n        return input","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.13436Z","iopub.execute_input":"2022-06-19T17:15:28.135007Z","iopub.status.idle":"2022-06-19T17:15:28.152876Z","shell.execute_reply.started":"2022-06-19T17:15:28.13496Z","shell.execute_reply":"2022-06-19T17:15:28.151696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST CASE","metadata":{}},{"cell_type":"code","source":"# td = TimeSeriesDatasetNpz(root_dir='../input/moving-mnist/mnist_test_seq.npy', n_frames_input=10, n_frames_output=10)\n# train_loader = torch.utils.data.DataLoader(dataset=td, batch_size=3, shuffle=True, num_workers=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.156765Z","iopub.execute_input":"2022-06-19T17:15:28.158137Z","iopub.status.idle":"2022-06-19T17:15:28.169977Z","shell.execute_reply.started":"2022-06-19T17:15:28.158102Z","shell.execute_reply":"2022-06-19T17:15:28.168958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# z = next(iter(train_loader))\n# print(z.shape)\n# print(torch.max(z), torch.min(z))","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.17185Z","iopub.execute_input":"2022-06-19T17:15:28.172429Z","iopub.status.idle":"2022-06-19T17:15:28.184717Z","shell.execute_reply.started":"2022-06-19T17:15:28.172385Z","shell.execute_reply":"2022-06-19T17:15:28.183593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure()\n# plt.imshow(z[0][0].permute(1, 2, 0)*0.5 + 0.5, cmap='gray') \n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.186756Z","iopub.execute_input":"2022-06-19T17:15:28.187526Z","iopub.status.idle":"2022-06-19T17:15:28.195247Z","shell.execute_reply.started":"2022-06-19T17:15:28.187477Z","shell.execute_reply":"2022-06-19T17:15:28.194174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODELS**","metadata":{}},{"cell_type":"markdown","source":"## STCell","metadata":{}},{"cell_type":"code","source":"class SpatioTemporalLSTMCell(nn.Module):\n    def __init__(self, in_channel, num_hidden, width, filter_size, stride, layer_norm):\n        super(SpatioTemporalLSTMCell, self).__init__()\n\n        self.num_hidden = num_hidden\n        self.padding = filter_size // 2\n        self._forget_bias = 1.0\n        if layer_norm:\n            self.conv_x = nn.Sequential(\n                nn.Conv2d(in_channel, num_hidden * 7, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n                nn.LayerNorm([num_hidden * 7, width, width])\n            )\n            self.conv_h = nn.Sequential(\n                nn.Conv2d(num_hidden, num_hidden * 4, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n                nn.LayerNorm([num_hidden * 4, width, width])\n            )\n            self.conv_m = nn.Sequential(\n                nn.Conv2d(num_hidden, num_hidden * 3, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n                nn.LayerNorm([num_hidden * 3, width, width])\n            )\n            self.conv_o = nn.Sequential(\n                nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n                nn.LayerNorm([num_hidden, width, width])\n            )\n        else:\n            self.conv_x = nn.Sequential(\n                nn.Conv2d(in_channel, num_hidden * 7, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n            )\n            self.conv_h = nn.Sequential(\n                nn.Conv2d(num_hidden, num_hidden * 4, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n            )\n            self.conv_m = nn.Sequential(\n                nn.Conv2d(num_hidden, num_hidden * 3, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n            )\n            self.conv_o = nn.Sequential(\n                nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),\n            )\n        self.conv_last = nn.Conv2d(num_hidden * 2, num_hidden, kernel_size=1, stride=1, padding=0, bias=False)\n\n    def forward(self, x_t, h_t, c_t, m_t):\n        x_concat = self.conv_x(x_t)\n        h_concat = self.conv_h(h_t)\n        m_concat = self.conv_m(m_t)\n        i_x, f_x, g_x, i_x_prime, f_x_prime, g_x_prime, o_x = torch.split(x_concat, self.num_hidden, dim=1)\n        i_h, f_h, g_h, o_h = torch.split(h_concat, self.num_hidden, dim=1)\n        i_m, f_m, g_m = torch.split(m_concat, self.num_hidden, dim=1)\n\n        i_t = torch.sigmoid(i_x + i_h)\n        f_t = torch.sigmoid(f_x + f_h + self._forget_bias)\n        g_t = torch.tanh(g_x + g_h)\n\n        c_new = f_t * c_t + i_t * g_t\n\n        i_t_prime = torch.sigmoid(i_x_prime + i_m)\n        f_t_prime = torch.sigmoid(f_x_prime + f_m + self._forget_bias)\n        g_t_prime = torch.tanh(g_x_prime + g_m)\n\n        m_new = f_t_prime * m_t + i_t_prime * g_t_prime\n\n        mem = torch.cat((c_new, m_new), 1)\n        o_t = torch.sigmoid(o_x + o_h + self.conv_o(mem))\n        h_new = o_t * torch.tanh(self.conv_last(mem))\n\n        return h_new, c_new, m_new\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.197018Z","iopub.execute_input":"2022-06-19T17:15:28.197392Z","iopub.status.idle":"2022-06-19T17:15:28.224537Z","shell.execute_reply.started":"2022-06-19T17:15:28.197345Z","shell.execute_reply":"2022-06-19T17:15:28.223512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MAU","metadata":{}},{"cell_type":"code","source":"class RNNZZ(nn.Module):\n    def __init__(self, num_layers, num_hidden, configs):\n        super(RNN, self).__init__()\n        # print(configs.srcnn_tf)\n        self.configs = configs\n        self.frame_channel = configs.patch_size * configs.patch_size * configs.img_channel\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.tau = configs.tau\n        self.cell_mode = configs.cell_mode\n        self.states = ['recall', 'normal']\n        if not self.configs.model_mode in self.states:\n            raise AssertionError\n        # self.time = 2\n        cell_list = []\n\n        width = configs.img_width // configs.patch_size // configs.sr_size\n        height = configs.img_height // configs.patch_size // configs.sr_size\n        # print(width)\n\n        for i in range(num_layers):\n            in_channel = num_hidden[i - 1]\n            cell_list.append(\n                MAUCell(in_channel, num_hidden[i], height, width, configs.filter_size,\n                        configs.stride, self.tau, self.cell_mode)\n            )\n        self.cell_list = nn.ModuleList(cell_list)\n\n        # Encoder\n        n = int(math.log2(configs.sr_size))\n        encoders = []\n        encoder = nn.Sequential()\n        encoder.add_module(name='encoder_t_conv{0}'.format(-1),\n                           module=nn.Conv2d(in_channels=self.frame_channel,\n                                            out_channels=self.num_hidden[0],\n                                            stride=1,\n                                            padding=0,\n                                            kernel_size=1))\n        encoder.add_module(name='relu_t_{0}'.format(-1),\n                           module=nn.LeakyReLU(0.2))\n        encoders.append(encoder)\n        for i in range(n):\n            encoder = nn.Sequential()\n            encoder.add_module(name='encoder_t{0}'.format(i),\n                               module=nn.Conv2d(in_channels=self.num_hidden[0],\n                                                out_channels=self.num_hidden[0],\n                                                stride=(2, 2),\n                                                padding=(1, 1),\n                                                kernel_size=(3, 3)\n                                                ))\n            # self.encoder_t.add_module(name='gn_t{0}'.format(i),\n            #                           module=nn.GroupNorm(4, self.frame_channel))\n            encoder.add_module(name='encoder_t_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            encoders.append(encoder)\n        self.encoders = nn.ModuleList(encoders)\n\n        # Decoder\n        decoders = []\n\n        for i in range(n - 1):\n            decoder = nn.Sequential()\n            decoder.add_module(name='c_decoder{0}'.format(i),\n                               module=nn.ConvTranspose2d(in_channels=self.num_hidden[-1],\n                                                         out_channels=self.num_hidden[-1],\n                                                         stride=(2, 2),\n                                                         padding=(1, 1),\n                                                         kernel_size=(3, 3),\n                                                         output_padding=(1, 1)\n                                                         ))\n            # self.decoder_s.add_module(name='gn_decoder_s{0}'.format(i),\n            #                           module=nn.GroupNorm(4, self.frame_channel))\n            decoder.add_module(name='c_decoder_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            decoders.append(decoder)\n\n        if n > 0:\n            decoder = nn.Sequential()\n            decoder.add_module(name='c_decoder{0}'.format(n - 1),\n                               module=nn.ConvTranspose2d(in_channels=self.num_hidden[-1],\n                                                         out_channels=self.num_hidden[-1],\n                                                         stride=(2, 2),\n                                                         padding=(1, 1),\n                                                         kernel_size=(3, 3),\n                                                         output_padding=(1, 1)\n                                                         ))\n            decoders.append(decoder)\n        self.decoders = nn.ModuleList(decoders)\n\n        self.srcnn = nn.Sequential(\n            nn.Conv2d(self.num_hidden[-1], self.frame_channel, kernel_size=1, stride=1, padding=0)\n        )\n        self.merge = nn.Conv2d(self.num_hidden[-1] * 2, self.num_hidden[-1], kernel_size=1, stride=1, padding=0)\n        self.conv_last_sr = nn.Conv2d(self.frame_channel * 2, self.frame_channel, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, frames, mask_true, verbose=False):\n        # print('ok')\n        mask_true = mask_true.permute(0, 1, 4, 2, 3).contiguous()\n        if(verbose):\n            print('MT Permuted to: ',  mask_true.shape)\n        batch_size = frames.shape[0]\n        height = frames.shape[3] // self.configs.sr_size\n        width = frames.shape[4] // self.configs.sr_size\n        frame_channels = frames.shape[2]\n        next_frames = []\n        T_t = []\n        T_pre = []\n        S_pre = []\n        # H_t = []\n        x_gen = None\n        if(verbose):\n            print('Num Layers: ', self.num_layers)\n            print('Num Hidden: ', self.num_hidden)\n            print('TAU: ', self.tau)\n        for layer_idx in range(self.num_layers):\n            tmp_t = []\n            tmp_s = []\n            if layer_idx == 0:\n                in_channel = self.num_hidden[layer_idx]\n            else:\n                in_channel = self.num_hidden[layer_idx - 1]\n            for i in range(self.tau):\n                if(verbose):\n                    if i==2:\n                        print('tmp_t[1]', tmp_t[1].shape)\n                tmp_t.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n                tmp_s.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n            T_pre.append(tmp_t)\n            S_pre.append(tmp_s)\n\n        if(verbose):\n            print('len T_pre', len(T_pre))\n            print('len T_pre[1]', len(T_pre[1]))\n\n        for t in range(self.configs.total_length - 1):\n            if t < self.configs.input_length:\n                net = frames[:, t]\n                if(verbose):\n                    print('Net frames[:, t]', frames[:, t].shape)\n            else:\n                time_diff = t - self.configs.input_length\n                net = mask_true[:, time_diff] * frames[:, t] + (1 - mask_true[:, time_diff]) * x_gen\n                if(verbose):\n                    print('Net', net.shape)\n\n            frames_feature = net\n            frames_feature_encoded = []\n            if(verbose):\n                print('Len Encoders', len(self.encoders))\n            for i in range(len(self.encoders)):\n                frames_feature = self.encoders[i](frames_feature)\n                if(verbose):\n                    print('Frames_feature', i, frames_feature.shape)\n                frames_feature_encoded.append(frames_feature)\n            if t == 0:\n                for i in range(self.num_layers):\n                    zeros = torch.zeros([batch_size, self.num_hidden[i], height, width]).to(self.configs.device)\n                    T_t.append(zeros)\n                    # print('ok')\n            S_t = frames_feature\n            if(verbose):\n                print('S_t in', S_t.shape)\n            for i in range(self.num_layers):\n                t_att = T_pre[i][-self.tau:]\n                t_att = torch.stack(t_att, dim=0)\n                s_att = S_pre[i][-self.tau:]\n                s_att = torch.stack(s_att, dim=0)\n                S_pre[i].append(S_t)\n                T_t[i], S_t = self.cell_list[i](T_t[i], S_t, t_att, s_att)\n                T_pre[i].append(T_t[i])\n                # S_pre[i].append(S_t)\n            out = S_t\n            if(verbose):\n                print('S_t out', S_t.shape)\n            # out = self.merge(torch.cat([T_t[-1], S_t], dim=1))\n            frames_feature_decoded = []\n            for i in range(len(self.decoders)):\n                out = self.decoders[i](out)\n                if(verbose):\n                    print('S_t out', i, out.shape)\n                # print(\"ok\")\n                if self.configs.model_mode == 'recall':\n                    # print('unet')\n                    out = out + frames_feature_encoded[-2 - i]\n                    if(verbose):\n                        print('S_t out unet', i, out.shape)\n            # out = self.decoder(out)\n\n            x_gen = self.srcnn(out)\n            next_frames.append(x_gen)\n            if(verbose):\n                print('x_gen', x_gen.shape)\n                print('len next_frames', len(next_frames))\n        if(verbose):\n            print('len next_frames FULL', len(next_frames))\n        next_frames = torch.stack(next_frames, dim=0)\n        if(verbose):\n            print('next_frames Tensor', next_frames.shape)\n        next_frames = next_frames.permute(1, 0, 2, 3, 4).contiguous()\n        if(verbose):\n            print('next_frames Tensor Permuted', next_frames.shape)\n        return next_frames","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.226329Z","iopub.execute_input":"2022-06-19T17:15:28.226877Z","iopub.status.idle":"2022-06-19T17:15:28.277154Z","shell.execute_reply.started":"2022-06-19T17:15:28.226831Z","shell.execute_reply":"2022-06-19T17:15:28.275929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, num_layers, num_hidden, configs):\n        super(RNN, self).__init__()\n\n        self.configs = configs\n        self.frame_channel = configs.patch_size * configs.patch_size * configs.img_channel\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        cell_list = []\n\n        width = configs.img_width // configs.patch_size\n        self.MSE_criterion = nn.MSELoss()\n\n        for i in range(num_layers):\n            in_channel = self.frame_channel if i == 0 else num_hidden[i - 1]\n            cell_list.append(\n                SpatioTemporalLSTMCell(in_channel, num_hidden[i], width, configs.filter_size,\n                                       configs.stride, configs.layer_norm)\n            )\n        self.cell_list = nn.ModuleList(cell_list)\n        self.conv_last = nn.Conv2d(num_hidden[num_layers - 1], self.frame_channel,\n                                   kernel_size=1, stride=1, padding=0, bias=False)\n\n    def forward(self, frames_tensor, mask_true):\n        # [batch, length, height, width, channel] -> [batch, length, channel, height, width]\n        #frames = frames_tensor.permute(0, 1, 4, 2, 3).contiguous()\n        frames = frames_tensor\n        mask_true = mask_true.permute(0, 1, 4, 2, 3).contiguous()\n\n        batch = frames.shape[0]\n        height = frames.shape[3]\n        width = frames.shape[4]\n\n        next_frames = []\n        h_t = []\n        c_t = []\n\n        for i in range(self.num_layers):\n            zeros = torch.zeros([batch, self.num_hidden[i], height, width]).to(self.configs.device)\n            h_t.append(zeros)\n            c_t.append(zeros)\n\n        memory = torch.zeros([batch, self.num_hidden[0], height, width]).to(self.configs.device)\n\n        for t in range(self.configs.total_length - 1):\n            # reverse schedule sampling\n            if self.configs.reverse_scheduled_sampling == 1:\n                if t == 0:\n                    net = frames[:, t]\n                else:\n                    net = mask_true[:, t - 1] * frames[:, t] + (1 - mask_true[:, t - 1]) * x_gen\n            else:\n                if t < self.configs.input_length:\n                    net = frames[:, t]\n                else:\n                    net = mask_true[:, t - self.configs.input_length] * frames[:, t] + \\\n                          (1 - mask_true[:, t - self.configs.input_length]) * x_gen\n\n            h_t[0], c_t[0], memory = self.cell_list[0](net, h_t[0], c_t[0], memory)\n\n            for i in range(1, self.num_layers):\n                h_t[i], c_t[i], memory = self.cell_list[i](h_t[i - 1], h_t[i], c_t[i], memory)\n\n            x_gen = self.conv_last(h_t[self.num_layers - 1])\n            next_frames.append(x_gen)\n\n        # [length, batch, channel, height, width] -> [batch, length, height, width, channel]\n        next_frames = torch.stack(next_frames, dim=0).permute(1, 0, 2, 3, 4).contiguous()\n        loss = self.MSE_criterion(next_frames, frames_tensor[:, 1:])\n        return next_frames, loss, loss","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.279357Z","iopub.execute_input":"2022-06-19T17:15:28.280343Z","iopub.status.idle":"2022-06-19T17:15:28.303986Z","shell.execute_reply.started":"2022-06-19T17:15:28.280298Z","shell.execute_reply":"2022-06-19T17:15:28.30285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAINER TESTER**","metadata":{}},{"cell_type":"code","source":"def train(model, ims, real_input_flag, configs, itr, val):\n    _, loss_l1, loss_l2 = model.train(ims, real_input_flag, itr, val)\n    if itr % configs.display_interval == 0:\n        print('Step: ' + str(itr),\n              'Training L1 loss: ' + str(loss_l1), 'Training L2 loss: ' + str(loss_l2))\n    return loss_l1, loss_l2","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.305935Z","iopub.execute_input":"2022-06-19T17:15:28.306903Z","iopub.status.idle":"2022-06-19T17:15:28.318554Z","shell.execute_reply.started":"2022-06-19T17:15:28.306857Z","shell.execute_reply":"2022-06-19T17:15:28.317545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation_proper(model, test_loader, configs, out_len=10):\n    print('Evaluating...')\n    \n    loss_fn = lpips.LPIPS(net='alex', spatial=True).to(configs.device)\n    mse_list = np.empty((len(test_loader), out_len))\n    mae_list = np.empty((len(test_loader), out_len))\n    ssim_list = np.empty((len(test_loader), out_len))\n    psnr_list = np.empty((len(test_loader), out_len))\n    lpips_list = np.empty((len(test_loader), out_len))\n    \n    total_mse = 0\n    total_mae = 0\n    \n    with torch.no_grad():\n        #model.eval()\n        for i, data in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n \n            batch_size = data.shape[0]\n            real_input_flag = np.zeros(\n                (batch_size,\n                 configs.total_length - configs.input_length - 1,\n                 configs.img_height // configs.patch_size,\n                 configs.img_width // configs.patch_size,\n                 configs.patch_size ** 2 * configs.img_channel))\n\n            img_gen = model.test(data, real_input_flag)\n            img_gen = img_gen.transpose(0, 1, 3, 4, 2)  # * 0.5 + 0.5\n            test_ims = data.detach().cpu().numpy().transpose(0, 1, 3, 4, 2)  # * 0.5 + 0.5\n            output_length = configs.total_length - configs.input_length\n            output_length = min(output_length, configs.total_length - 1)\n            test_ims = reshape_patch_back(test_ims, configs.patch_size)\n            img_gen = reshape_patch_back(img_gen, configs.patch_size)\n            target = data[:, configs.input_length:, :].detach().cpu().numpy().transpose(0, 1, 3, 4, 2)\n            predictions = img_gen[:, -output_length:, :]\n            \n            predictions[predictions < 0.10] = 0\n            predictions[predictions > 0.99] = 1\n            \n            if (i+1) % 500 == 0:\n                print(target[0, 1, 40:42, 40:42, 0])\n                print(predictions[0, 1, 40:42, 40:42, 0])\n                fig, ax = plt.subplots(2, out_len, figsize=(25, 7))\n                for i in range(2):\n                    for j in range(out_len):\n                        if i == 0:\n                            ax[i][j].imshow(target[0][j])\n                            ax[i][j].set_title('V Ground Truth')\n                        if i == 1:\n                            ax[i][j].imshow(predictions[0][j])\n                            ax[i][j].set_title('V Generated')\n                        ax[i][j].axis('off')\n                plt.show()\n            \n            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,4)).sum()\n            mae_batch = np.mean(np.abs(predictions-target),  axis=(0,1,4)).sum() \n            total_mse += mse_batch\n            total_mae += mae_batch\n            \n            for j in range(out_len):\n                mse_list[i][j] = np.square(predictions[:,j,:,:,:] - target[:,j,:,:,:]).mean()\n                mae_list[i][j] = np.abs(predictions[:,j,:,:,:] - target[:,j,:,:,:]).mean()\n                ssim_list[i][j] = ssim(target[0,j,:,:,0], predictions[0,j,:,:,0], multichannel=False)\n                psnr_list[i][j] = 20 * np.log10(1 / sqrt(mse_list[i][j]))\n                t1 = torch.from_numpy((predictions[:,j,:,:,:] - 0.5) / 0.5).to(configs.device).permute((0, 3, 1, 2))\n                t2 = torch.from_numpy((target[:,j,:,:,:] - 0.5) / 0.5).to(configs.device).permute((0, 3, 1, 2))\n                d = loss_fn.forward(t1, t2)\n                lpips_list[i][j] = d.mean().detach().cpu().numpy() * 100\n                    \n        #model.train()\n        \n    avg_mse_frame = mse_list.mean(axis=0)\n    avg_mae_frame = mae_list.mean(axis=0)\n    avg_ssim_frame = ssim_list.mean(axis=0)\n    avg_psnr_frame = psnr_list.mean(axis=0)\n    avg_lpips_frame = lpips_list.mean(axis=0)\n\n    avg_mse = mse_list.mean()\n    avg_mae = mae_list.mean()\n    avg_ssim = ssim_list.mean()\n    avg_psnr = psnr_list.mean()\n    avg_lpips = lpips_list.mean()\n\n    print('Eval MSE: ', total_mse/(len(test_loader)))\n    print('Eval MAE: ', total_mae/(len(test_loader)))\n    \n    print(f'Avg-MSE: {avg_mse}\\nMSE/Frame: {avg_mse_frame}')\n    print(f'Avg-MAE: {avg_mae}\\nMAE/Frame: {avg_mae_frame}')\n    print(f'Avg-SSIM: {avg_ssim}\\nSSIM/Frame: {avg_ssim_frame}')\n    print(f'Avg-PSNR: {avg_psnr}\\nPSNR/Frame: {avg_psnr_frame}')\n    print(f'Avg-LPIPS: {avg_lpips}\\nLPIPS/Frame: {avg_lpips_frame}')\n    \n    return avg_mse","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.320363Z","iopub.execute_input":"2022-06-19T17:15:28.321454Z","iopub.status.idle":"2022-06-19T17:15:28.357226Z","shell.execute_reply.started":"2022-06-19T17:15:28.321396Z","shell.execute_reply":"2022-06-19T17:15:28.355846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN TEST WRAPPER**","metadata":{}},{"cell_type":"code","source":"def train_wrapper(model):\n    begin = 0\n    \n    if args.pretrained_model:\n        model.load(args.pretrained_model)\n        begin = int(args.pretrained_model.split('-')[-1])\n\n        \n    # DATASET\n    dataset = TimeSeriesDataset(root_dir=configs.data_train_path, n_frames_input=10, n_frames_output=10)\n    \n    # DATA LOADER + SPLIT\n    validation_split = .3\n    shuffle_dataset = True\n    random_seed= 1000\n\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n    \n    model.load('../input/predv1caswts/predv1forcasia.ckpt-4000')\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n    train_input_handle = torch.utils.data.DataLoader(dataset, args.batch_size, sampler=train_sampler, num_workers=2, pin_memory=True, drop_last=True)\n    val_input_handle = torch.utils.data.DataLoader(dataset, batch_size=4, sampler=valid_sampler, drop_last=True)\n\n    losses_l1 = []\n    losses_l2 = []\n    \n    eta = args.sampling_start_value\n    eta -= (begin * args.sampling_changing_rate)\n    itr = begin\n    # real_input_flag = {}\n    for epoch in range(0, args.max_epoches):\n        print('E:', epoch)\n            \n        if epoch == 0:\n            evaluation_proper(model, val_input_handle, configs, out_len=10)\n        \n        for ims in tqdm(train_input_handle, total=len(train_input_handle)):\n            if itr > args.max_iterations:\n                break\n            batch_size = ims.shape[0]\n            if(configs.verbose):\n                print('IMS shape: ', ims.shape)\n                print('Stuff input to schedule sampling: ', eta, itr, args.img_channel, batch_size)\n            eta, real_input_flag = schedule_sampling(eta, itr)\n            if(configs.verbose):\n                print('Stuff output from schedule sampling: ', eta, real_input_flag.shape)\n\n            l1, l2 = train(model, ims, real_input_flag, args, itr, next(iter(val_input_handle)))\n            losses_l1.append(l1.item())\n            losses_l2.append(l2.item())\n            \n            if itr % configs.plot_interval == 0:\n                fig, ax = plt.subplots(2, 1, figsize=(13, 5))\n                a = ax.flatten()\n                a[0].plot(losses_l1, 'r')\n                a[0].set_title('Loss L1')\n                a[1].plot(losses_l2, 'r')\n                a[1].set_title('Loss L2')\n                plt.show()\n            \n            if itr % args.snapshot_interval == 0 and itr > begin:\n                model.save(itr)\n            itr += 1\n\n        if epoch >= 8:\n            evaluation_proper(model, val_input_handle, configs, out_len=10)\n            \n        #evaluation_proper(model, val_input_handle, configs, out_len=10)\n            \n#             meminfo_end = pynvml.nvmlDeviceGetMemoryInfo(handle)\n#             if(configs.verbose):\n#                 print(\"GPU memory:%dM\" % ((meminfo_end.used - meminfo_begin.used) / (1024 ** 2)))\n\n\ndef test_wrapper(model, val_ds):\n    #model.load(args.pretrained_model)\n    test_input_handle = val_ds\n\n#     itr = 1\n#     for i in range(itr):\n    trainer.test(model, test_input_handle, args, itr)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.359199Z","iopub.execute_input":"2022-06-19T17:15:28.359918Z","iopub.status.idle":"2022-06-19T17:15:28.394526Z","shell.execute_reply.started":"2022-06-19T17:15:28.359867Z","shell.execute_reply":"2022-06-19T17:15:28.393121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL FACTORY**","metadata":{}},{"cell_type":"code","source":"class Model(object):\n    def __init__(self, configs):\n        self.configs = configs\n        self.patch_height = configs.img_height // configs.patch_size\n        self.patch_width = configs.img_width // configs.patch_size\n        self.patch_channel = configs.img_channel * (configs.patch_size ** 2)\n        self.num_layers = configs.num_layers\n        networks_map = {\n            'predv2': RNN,\n        }\n        num_hidden = []\n        for i in range(configs.num_layers):\n            num_hidden.append(configs.num_hidden)\n        self.num_hidden = num_hidden\n        if configs.model_name in networks_map:\n            Network = networks_map[configs.model_name]\n            self.network = Network(self.num_layers, self.num_hidden, configs).to(configs.device)\n        else:\n            raise ValueError('Name of network unknown %s' % configs.model_name)\n\n        self.optimizer = Adam(self.network.parameters(), lr=configs.lr)\n        self.MSE_criterion = nn.MSELoss()\n        self.L1_loss = nn.L1Loss()\n\n    def save(self, itr):\n        stats = {'net_param': self.network.state_dict()}\n        checkpoint_path = os.path.join(self.configs.save_dir, 'model.ckpt' + '-' + str(itr))\n        torch.save(stats, checkpoint_path)\n        print(\"save predictive model to %s\" % checkpoint_path)\n\n    def load(self, pm_checkpoint_path):\n        print('load predictive model:', pm_checkpoint_path)\n        stats = torch.load(pm_checkpoint_path, map_location=torch.device(self.configs.device))\n        self.network.load_state_dict(stats['net_param'])\n\n    def train(self, data, mask, itr, val):\n        frames = data\n        self.network.train()\n        val_tensor = torch.FloatTensor(val).to(self.configs.device)\n        frames_tensor = torch.FloatTensor(frames).to(self.configs.device)\n        mask_tensor = torch.FloatTensor(mask).to(self.configs.device)\n\n        if(self.configs.verbose):\n            print('FT', frames_tensor.shape)\n            print('MT', mask_tensor.shape)\n        \n        next_frames, loss1, loss2 = self.network(frames_tensor, mask_tensor)\n        loss = loss1 + loss2\n        if(self.configs.verbose):\n            print('Next Frames', next_frames.shape)\n            \n        #next_frames = next_frames.permute(0, 1, 4, 2, 3)\n\n        ground_truth = frames_tensor\n        if(self.configs.verbose):\n            print('Ground', ground_truth[:, 1:].shape)\n\n            \n        \n        if itr % configs.plot_interval == 0:\n            with torch.no_grad():\n                self.network.eval()\n                x = frames_tensor[0][0:configs.input_length]\n                y = frames_tensor[0][configs.input_length:]\n                g = next_frames[0][configs.input_length-1:]\n                m = mask_tensor[0]\n                fig, ax = plt.subplots(4, configs.input_length, figsize=(25, 10))\n                for i in range(4):\n                    for j in range(configs.input_length):\n                        if i == 0:\n                            ax[i][j].imshow(x[j].to('cpu').permute(1, 2, 0))\n                            ax[i][j].set_title('T Input')\n                        if i == 1:\n                            if j == configs.input_length-1:\n                                ax[i][j].axis('off')\n                                continue\n                            ax[i][j].imshow(m[j].to('cpu'))\n                            ax[i][j].set_title('T Mask')\n                        if i == 2:\n                            ax[i][j].imshow(y[j].to('cpu').permute(1, 2, 0))\n                            ax[i][j].set_title('T Ground Truth')\n                        if i == 3:\n                            ax[i][j].imshow(g[j].detach().to('cpu').permute(1, 2, 0))\n                            ax[i][j].set_title('T Generated')\n                        ax[i][j].axis('off')\n\n\n#                 x = val_tensor[0][0:configs.input_length]\n#                 y = val_tensor[0][configs.input_length:]\n#                 mask = torch.zeros_like(mask_tensor[0]).unsqueeze(0).to(configs.device)\n#                 next_frameszz = self.network(val_tensor, mask)\n#                 m = mask[0]\n#                 g = next_frameszz[0][configs.input_length-1:]\n#                 fig, ax = plt.subplots(4, configs.input_length, figsize=(25, 10))\n#                 for i in range(4):\n#                     for j in range(configs.input_length):\n#                         if i == 0:\n#                             ax[i][j].imshow(x[j].to('cpu').permute(1, 2, 0))\n#                             ax[i][j].set_title('V Input')\n#                         if i == 1:\n#                             if j == configs.input_length-1:\n#                                 ax[i][j].axis('off')\n#                                 continue\n#                             ax[i][j].imshow(m[j].to('cpu'))\n#                             ax[i][j].set_title('V Mask')\n#                         if i == 2:\n#                             ax[i][j].imshow(y[j].to('cpu').permute(1, 2, 0))\n#                             ax[i][j].set_title('V Ground Truth')\n#                         if i == 3:\n#                             ax[i][j].imshow(g[j].detach().to('cpu').permute(1, 2, 0))\n#                             ax[i][j].set_title('V Generated')\n#                         ax[i][j].axis('off')\n                \n                self.network.train()\n            \n                    \n            \n        batch_size = next_frames.shape[0]\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n#         if itr >= self.configs.sampling_stop_iter and itr % self.configs.delay_interval == 0:\n#             self.scheduler.step()\n#             # self.scheduler_F.step()\n#             # self.scheduler_D.step()\n#             print('LR decay to:%.8f', self.optimizer.param_groups[0]['lr'])\n\n        return next_frames, loss1.detach().cpu().numpy(), loss2.detach().cpu().numpy()\n\n    def test(self, data, mask):\n        frames = data\n        self.network.eval()\n        frames_tensor = torch.FloatTensor(frames).to(self.configs.device)\n        mask_tensor = torch.FloatTensor(mask).to(self.configs.device)\n        next_frames, loss1, loss2 = self.network(frames_tensor, mask_tensor)\n        return next_frames.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.396454Z","iopub.execute_input":"2022-06-19T17:15:28.396927Z","iopub.status.idle":"2022-06-19T17:15:28.434046Z","shell.execute_reply.started":"2022-06-19T17:15:28.396884Z","shell.execute_reply":"2022-06-19T17:15:28.43287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CONFIG**","metadata":{}},{"cell_type":"code","source":"class Configuration:\n    def __init__(self):\n        super(Configuration, self).__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.data_train_path = '../input/casiabpretreated/Here'\n        self.data_test_path = '../input/kthextract-to-jpg/data/test'\n        self.input_length = 10\n        self.real_length = 20\n        self.total_length = 20\n        self.img_height = 64\n        self.img_width = 64\n        self.sr_size = 4\n        self.img_channel = 1\n        self.patch_size = 1\n        self.alpha = 1\n        self.model_name = 'predv2'\n        self.dataset = 'mmnist'\n        self.cell_mode = 'residual'\n        self.model_mode = 'recall'\n        self.num_workers = 2\n        self.num_hidden = 32\n        self.num_layers = 2\n        self.num_heads = 2\n        self.filter_size = 5\n        self.stride = 1\n        self.time = 2\n        self.time_stride = 1\n        self.tau = 7\n        self.is_training = True\n        self.lr = 1e-3\n        self.lr_decay = 0.90\n        self.delay_interval = 2000\n        self.batch_size = 32\n        self.max_iterations = 150000\n        self.max_epoches = 10\n        self.display_interval = 150\n        self.plot_interval = 150\n        self.test_interval = 1010\n        self.snapshot_interval = 1000\n        self.num_save_samples = 3\n        self.n_gpu = 1\n        self.pretrained_model = ''\n        self.perforamnce_dir = 'results/mmnist'\n        self.save_dir = 'saves/mmnist'\n        self.gen_frm_dir = 'results/mmnist/'\n        self.scheduled_sampling = True\n        self.sampling_stop_iter = 50000\n        self.sampling_start_value = 1.0\n        self.sampling_changing_rate = 0.000005\n        self.verbose = False\n        self.visual = 0\n        self.visual_path=''\n        self.layer_norm=0\n        self.reverse_scheduled_sampling=0\n        self.decouple_beta=0.1\n        \nconfigs = Configuration()\nargs = configs","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.435768Z","iopub.execute_input":"2022-06-19T17:15:28.4366Z","iopub.status.idle":"2022-06-19T17:15:28.454261Z","shell.execute_reply.started":"2022-06-19T17:15:28.436534Z","shell.execute_reply":"2022-06-19T17:15:28.453138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST CASE","metadata":{}},{"cell_type":"code","source":"def test_model(configs):\n    nl = 4\n    nh = [64, 64, 64, 64]\n    z = torch.randn(1, 20, 1, 256, 256).to(configs.device)\n    m = torch.zeros(1, 9, 256, 256, 1).to(configs.device)\n    model = RNN(nl, nh, configs).to(configs.device)\n    g = model(z, m, True)\n    print(g.shape)\n\n#test_model(configs)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.455869Z","iopub.execute_input":"2022-06-19T17:15:28.456313Z","iopub.status.idle":"2022-06-19T17:15:28.469424Z","shell.execute_reply.started":"2022-06-19T17:15:28.456267Z","shell.execute_reply":"2022-06-19T17:15:28.46807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = Model(args)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.471244Z","iopub.execute_input":"2022-06-19T17:15:28.471587Z","iopub.status.idle":"2022-06-19T17:15:28.482871Z","shell.execute_reply.started":"2022-06-19T17:15:28.471543Z","shell.execute_reply":"2022-06-19T17:15:28.48185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count_parameters(model.network)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.484874Z","iopub.execute_input":"2022-06-19T17:15:28.485585Z","iopub.status.idle":"2022-06-19T17:15:28.493071Z","shell.execute_reply.started":"2022-06-19T17:15:28.485508Z","shell.execute_reply":"2022-06-19T17:15:28.492028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN**","metadata":{}},{"cell_type":"code","source":"#pynvml.nvmlInit()\n\nprint('Initializing models')\n\nmodel = Model(args)\n\nif args.is_training:\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n    if not os.path.exists(args.gen_frm_dir):\n        os.makedirs(args.gen_frm_dir)\n    train_wrapper(model)\nelse:\n    if not os.path.exists(args.gen_frm_dir):\n        os.makedirs(args.gen_frm_dir)\n    test_wrapper(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T17:15:28.49604Z","iopub.execute_input":"2022-06-19T17:15:28.496773Z"},"trusted":true},"execution_count":null,"outputs":[]}]}