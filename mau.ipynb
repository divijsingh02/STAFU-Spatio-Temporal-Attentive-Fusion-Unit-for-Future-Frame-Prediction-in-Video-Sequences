{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IMPORTS**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.utils.data as data\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport numpy as np\nfrom tqdm import tqdm\nfrom numpy import *\nfrom numpy.linalg import *\nfrom scipy.special import factorial\nfrom functools import reduce\nimport random\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport time\nimport gzip\nimport cv2\nimport math\nimport os\nfrom PIL import Image\nfrom skimage.metrics import structural_similarity as compare_ssim\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.transform import resize\nimport argparse\n# !pip install lpips\nimport codecs\n# import lpips\n#!pip install pynvml\n#import pynvml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY**","metadata":{}},{"cell_type":"code","source":"def reshape_patch(img_tensor, patch_size):\n    assert 4 == img_tensor.ndim\n    seq_length = np.shape(img_tensor)[0]\n    img_height = np.shape(img_tensor)[1]\n    img_width = np.shape(img_tensor)[2]\n    num_channels = np.shape(img_tensor)[3]\n    a = np.reshape(img_tensor, [seq_length,\n                                img_height // patch_size, patch_size,\n                                img_width // patch_size, patch_size,\n                                num_channels])\n    b = np.transpose(a, [0, 1, 3, 2, 4, 5])\n    patch_tensor = np.reshape(b, [seq_length,\n                                  img_height // patch_size,\n                                  img_width // patch_size,\n                                  patch_size * patch_size * num_channels])\n    return patch_tensor\n\n\ndef reshape_patch_back(patch_tensor, patch_size):\n    # B L H W C\n    assert 5 == patch_tensor.ndim\n    batch_size = np.shape(patch_tensor)[0]\n    seq_length = np.shape(patch_tensor)[1]\n    patch_height = np.shape(patch_tensor)[2]\n    patch_width = np.shape(patch_tensor)[3]\n    channels = np.shape(patch_tensor)[4]\n    img_channels = channels // (patch_size * patch_size)\n    a = np.reshape(patch_tensor, [batch_size, seq_length,\n                                  patch_height, patch_width,\n                                  patch_size, patch_size,\n                                  img_channels])\n    b = np.transpose(a, [0, 1, 2, 4, 3, 5, 6])\n    img_tensor = np.reshape(b, [batch_size, seq_length,\n                                patch_height * patch_size,\n                                patch_width * patch_size,\n                                img_channels])\n    return img_tensor\n\n\ndef reshape_patch_back_tensor(patch_tensor, patch_size):\n    # B L H W C\n    assert 5 == patch_tensor.ndim\n    patch_narray = patch_tensor.detach().cpu().numpy()\n    batch_size = np.shape(patch_narray)[0]\n    seq_length = np.shape(patch_narray)[1]\n    patch_height = np.shape(patch_narray)[2]\n    patch_width = np.shape(patch_narray)[3]\n    channels = np.shape(patch_narray)[4]\n    img_channels = channels // (patch_size * patch_size)\n    a = torch.reshape(patch_tensor, [batch_size, seq_length,\n                                     patch_height, patch_width,\n                                     patch_size, patch_size,\n                                     img_channels])\n    b = a.permute([0, 1, 2, 4, 3, 5, 6])\n    img_tensor = torch.reshape(b, [batch_size, seq_length,\n                                   patch_height * patch_size,\n                                   patch_width * patch_size,\n                                   img_channels])\n    return img_tensor.permute(0, 1, 4, 2, 3)\n\n\ndef reshape_patch_tensor(img_tensor, patch_size):\n    assert 4 == img_tensor.ndim\n    seq_length = img_tensor.shape[0]\n    img_height = img_tensor.shape[1]\n    img_width = img_tensor.shape[2]\n    num_channels = img_tensor.shape[3]\n    a = torch.reshape(img_tensor, [seq_length,\n                                   img_height // patch_size, patch_size,\n                                   img_width // patch_size, patch_size,\n                                   num_channels])\n    b = a.permute((0, 1, 3, 2, 4, 5))\n    patch_tensor = torch.reshape(b, [seq_length,\n                                     img_height // patch_size,\n                                     img_width // patch_size,\n                                     patch_size * patch_size * num_channels])\n    return patch_tensor.permute((0, 3, 1, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DATA LOADER**","metadata":{}},{"cell_type":"code","source":"class Norm(object):\n    def __init__(self, max=255):\n        self.max = max\n\n    def __call__(self, sample):\n        video_x = sample\n        new_video_x = video_x / self.max\n        return new_video_x\n\n\nclass ToTensor(object):\n\n    def __call__(self, sample):\n        video_x = sample\n        video_x = video_x.transpose((0, 3, 1, 2))\n        video_x = np.array(video_x)\n        return torch.from_numpy(video_x).float()\n    \n\nclass Resize(object):\n\n    def __call__(self, sample):\n        imgs_out = np.zeros((\n            sample.shape[0], configs.img_height, configs.img_width, sample.shape[3]))\n        for i in range(sample.shape[0]):\n            imgs_out[i,:,:,:] = resize(sample[i,:,:,:], imgs_out.shape[1:])\n        return imgs_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesDataset(data.Dataset):\n    def __init__(self, root_dir, n_frames_input=10, n_frames_output=10):\n        self.n_frames_in = n_frames_input\n        self.n_frames_out = n_frames_output\n        random.seed(420)\n        n_frames = n_frames_input + n_frames_output\n        \n        self.file = np.load(root_dir).transpose(1,0,2,3)[..., np.newaxis].transpose(0,1,4,2,3)#[:10]        \n            \n            \n    def __len__(self):\n        return len(self.file)\n\n    def __getitem__(self, index):\n        clips = torch.from_numpy(self.file[index])\n        clips = clips.type(torch.float32)\n        clips = (clips / 255)\n        return clips","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"td = TimeSeriesDataset(root_dir='../input/moving-mnist/mnist_test_seq.npy', n_frames_input=10, n_frames_output=10)\ntrain_loader = torch.utils.data.DataLoader(dataset=td, batch_size=1, shuffle=True, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = next(iter(train_loader))\nprint(z.shape)\ntorch.max(z), torch.min(z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODELS**","metadata":{}},{"cell_type":"markdown","source":"## MAU-CELL","metadata":{}},{"cell_type":"code","source":"class MAUCell(nn.Module):\n    def __init__(self, in_channel, num_hidden, height, width, filter_size, stride, tau, cell_mode):\n        super(MAUCell, self).__init__()\n\n        self.num_hidden = num_hidden\n        self.padding = (filter_size[0] // 2, filter_size[1] // 2)\n        self.cell_mode = cell_mode\n        self.d = num_hidden * height * width\n        self.tau = tau\n        self.states = ['residual', 'normal']\n        if not self.cell_mode in self.states:\n            raise AssertionError\n        self.conv_t = nn.Sequential(\n            nn.Conv2d(in_channel, 3 * num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding,\n                      ),\n            nn.LayerNorm([3 * num_hidden, height, width])\n        )\n        self.conv_t_next = nn.Sequential(\n            nn.Conv2d(in_channel, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding,\n                      ),\n            nn.LayerNorm([num_hidden, height, width])\n        )\n        self.conv_s = nn.Sequential(\n            nn.Conv2d(num_hidden, 3 * num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding,\n                      ),\n            nn.LayerNorm([3 * num_hidden, height, width])\n        )\n        self.conv_s_next = nn.Sequential(\n            nn.Conv2d(num_hidden, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding,\n                      ),\n            nn.LayerNorm([num_hidden, height, width])\n        )\n        self.softmax = nn.Softmax(dim=0)\n\n    def forward(self, T_t, S_t, t_att, s_att):\n        s_next = self.conv_s_next(S_t)\n        t_next = self.conv_t_next(T_t)\n        weights_list = []\n        for i in range(self.tau):\n            weights_list.append((s_att[i] * s_next).sum(dim=(1, 2, 3)) / math.sqrt(self.d))\n        weights_list = torch.stack(weights_list, dim=0)\n        weights_list = torch.reshape(weights_list, (*weights_list.shape, 1, 1, 1))\n        weights_list = self.softmax(weights_list)\n        T_trend = t_att * weights_list\n        T_trend = T_trend.sum(dim=0)\n        t_att_gate = torch.sigmoid(t_next)\n        T_fusion = T_t * t_att_gate + (1 - t_att_gate) * T_trend\n        T_concat = self.conv_t(T_fusion)\n        S_concat = self.conv_s(S_t)\n        t_g, t_t, t_s = torch.split(T_concat, self.num_hidden, dim=1)\n        s_g, s_t, s_s = torch.split(S_concat, self.num_hidden, dim=1)\n        T_gate = torch.sigmoid(t_g)\n        S_gate = torch.sigmoid(s_g)\n        T_new = T_gate * t_t + (1 - T_gate) * s_t\n        S_new = S_gate * s_s + (1 - S_gate) * t_s\n        if self.cell_mode == 'residual':\n            S_new = S_new + S_t\n        return T_new, S_new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MAU","metadata":{}},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, num_layers, num_hidden, configs):\n        super(RNN, self).__init__()\n        self.configs = configs\n        self.frame_channel = configs.patch_size * configs.patch_size * configs.img_channel\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.tau = configs.tau\n        self.cell_mode = configs.cell_mode\n        self.states = ['recall', 'normal']\n        if not self.configs.model_mode in self.states:\n            raise AssertionError\n        cell_list = []\n\n        width = configs.img_width // configs.patch_size // configs.sr_size\n        height = configs.img_height // configs.patch_size // configs.sr_size\n\n        for i in range(num_layers):\n            in_channel = num_hidden[i - 1]\n            cell_list.append(\n                MAUCell(in_channel, num_hidden[i], height, width, configs.filter_size,\n                        configs.stride, self.tau, self.cell_mode)\n            )\n        self.cell_list = nn.ModuleList(cell_list)\n\n        # Encoder\n        n = int(math.log2(configs.sr_size))\n        encoders = []\n        encoder = nn.Sequential()\n        encoder.add_module(name='encoder_t_conv{0}'.format(-1),\n                           module=nn.Conv2d(in_channels=self.frame_channel,\n                                            out_channels=self.num_hidden[0],\n                                            stride=1,\n                                            padding=0,\n                                            kernel_size=1))\n        encoder.add_module(name='relu_t_{0}'.format(-1),\n                           module=nn.LeakyReLU(0.2))\n        encoders.append(encoder)\n        for i in range(n):\n            encoder = nn.Sequential()\n            encoder.add_module(name='encoder_t{0}'.format(i),\n                               module=nn.Conv2d(in_channels=self.num_hidden[0],\n                                                out_channels=self.num_hidden[0],\n                                                stride=(2, 2),\n                                                padding=(1, 1),\n                                                kernel_size=(3, 3)\n                                                ))\n            encoder.add_module(name='encoder_t_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            encoders.append(encoder)\n        self.encoders = nn.ModuleList(encoders)\n\n        # Decoder\n        decoders = []\n\n        for i in range(n - 1):\n            decoder = nn.Sequential()\n            decoder.add_module(name='c_decoder{0}'.format(i),\n                               module=nn.ConvTranspose2d(in_channels=self.num_hidden[-1],\n                                                         out_channels=self.num_hidden[-1],\n                                                         stride=(2, 2),\n                                                         padding=(1, 1),\n                                                         kernel_size=(3, 3),\n                                                         output_padding=(1, 1)\n                                                         ))\n            decoder.add_module(name='c_decoder_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            decoders.append(decoder)\n\n        if n > 0:\n            decoder = nn.Sequential()\n            decoder.add_module(name='c_decoder{0}'.format(n - 1),\n                               module=nn.ConvTranspose2d(in_channels=self.num_hidden[-1],\n                                                         out_channels=self.num_hidden[-1],\n                                                         stride=(2, 2),\n                                                         padding=(1, 1),\n                                                         kernel_size=(3, 3),\n                                                         output_padding=(1, 1)\n                                                         ))\n            decoders.append(decoder)\n        self.decoders = nn.ModuleList(decoders)\n\n        self.srcnn = nn.Sequential(\n            nn.Conv2d(self.num_hidden[-1], self.frame_channel, kernel_size=1, stride=1, padding=0)\n        )\n        self.merge = nn.Conv2d(self.num_hidden[-1] * 2, self.num_hidden[-1], kernel_size=1, stride=1, padding=0)\n        self.conv_last_sr = nn.Conv2d(self.frame_channel * 2, self.frame_channel, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, frames, mask_true):\n        # print('ok')\n        mask_true = mask_true.permute(0, 1, 4, 2, 3).contiguous()\n        batch_size = frames.shape[0]\n        height = frames.shape[3] // self.configs.sr_size\n        width = frames.shape[4] // self.configs.sr_size\n        frame_channels = frames.shape[2]\n        next_frames = []\n        T_t = []\n        T_pre = []\n        S_pre = []\n        x_gen = None\n        for layer_idx in range(self.num_layers):\n            tmp_t = []\n            tmp_s = []\n            if layer_idx == 0:\n                in_channel = self.num_hidden[layer_idx]\n            else:\n                in_channel = self.num_hidden[layer_idx - 1]\n            for i in range(self.tau):\n                tmp_t.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n                tmp_s.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n            T_pre.append(tmp_t)\n            S_pre.append(tmp_s)\n\n        for t in range(self.configs.total_length - 1):\n            if t < self.configs.input_length:\n                net = frames[:, t]\n            else:\n                time_diff = t - self.configs.input_length\n                net = mask_true[:, time_diff] * frames[:, t] + (1 - mask_true[:, time_diff]) * x_gen\n            frames_feature = net\n            frames_feature_encoded = []\n            for i in range(len(self.encoders)):\n                frames_feature = self.encoders[i](frames_feature)\n                frames_feature_encoded.append(frames_feature)\n            if t == 0:\n                for i in range(self.num_layers):\n                    zeros = torch.zeros([batch_size, self.num_hidden[i], height, width]).to(self.configs.device)\n                    T_t.append(zeros)\n            S_t = frames_feature\n            for i in range(self.num_layers):\n                t_att = T_pre[i][-self.tau:]\n                t_att = torch.stack(t_att, dim=0)\n                s_att = S_pre[i][-self.tau:]\n                s_att = torch.stack(s_att, dim=0)\n                S_pre[i].append(S_t)\n                T_t[i], S_t = self.cell_list[i](T_t[i], S_t, t_att, s_att)\n                T_pre[i].append(T_t[i])\n            out = S_t\n            # out = self.merge(torch.cat([T_t[-1], S_t], dim=1))\n            frames_feature_decoded = []\n            for i in range(len(self.decoders)):\n                out = self.decoders[i](out)\n                if self.configs.model_mode == 'recall':\n                    out = out + frames_feature_encoded[-2 - i]\n\n            x_gen = self.srcnn(out)\n            next_frames.append(x_gen)\n        next_frames = torch.stack(next_frames, dim=0).permute(1, 0, 2, 3, 4).contiguous()\n        return next_frames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAINER TESTER**","metadata":{}},{"cell_type":"code","source":"def train(model, ims, real_input_flag, configs, itr, val):\n    _, loss_l1, loss_l2 = model.train(ims, real_input_flag, itr, val)\n    if itr % configs.display_interval == 0:\n        print('Step: ' + str(itr),\n              'Training L1 loss: ' + str(loss_l1), 'Training L2 loss: ' + str(loss_l2))\n    return loss_l1, loss_l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation_proper(model, test_loader, configs, out_len=10):\n    print('Evaluating...')\n    \n    loss_fn = lpips.LPIPS(net='alex', spatial=True).to(configs.device)\n    mse_list = np.empty((len(test_loader), out_len))\n    mae_list = np.empty((len(test_loader), out_len))\n    ssim_list = np.empty((len(test_loader), out_len))\n    psnr_list = np.empty((len(test_loader), out_len))\n    lpips_list = np.empty((len(test_loader), out_len))\n    \n    total_mse = 0\n    total_mae = 0\n    \n    with torch.no_grad():\n        #model.eval()\n        for i, data in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n            batch_size = data.shape[0]\n            real_input_flag = np.zeros(\n                (batch_size,\n                 configs.total_length - configs.input_length - 1,\n                 configs.img_height // configs.patch_size,\n                 configs.img_width // configs.patch_size,\n                 configs.patch_size ** 2 * configs.img_channel))\n\n            img_gen = model.test(data, real_input_flag)\n            img_gen = img_gen.transpose(0, 1, 3, 4, 2)  # * 0.5 + 0.5\n            test_ims = data.detach().cpu().numpy().transpose(0, 1, 3, 4, 2)  # * 0.5 + 0.5\n            output_length = configs.total_length - configs.input_length\n            output_length = min(output_length, configs.total_length - 1)\n            test_ims = reshape_patch_back(test_ims, configs.patch_size)\n            img_gen = reshape_patch_back(img_gen, configs.patch_size)\n            target = data[:, configs.input_length:, :].detach().cpu().numpy().transpose(0, 1, 3, 4, 2)\n            predictions = img_gen[:, -output_length:, :]\n            \n            if (i+1) % 500 == 0:\n                print(target[0, 1, 40:42, 40:42, 0])\n                print(predictions[0, 1, 40:42, 40:42, 0])\n                fig, ax = plt.subplots(2, out_len, figsize=(25, 7))\n                for i in range(2):\n                    for j in range(out_len):\n                        if i == 0:\n                            ax[i][j].imshow(target[0][j])\n                            ax[i][j].set_title('V Ground Truth')\n                        if i == 1:\n                            ax[i][j].imshow(predictions[0][j])\n                            ax[i][j].set_title('V Generated')\n                        ax[i][j].axis('off')\n                plt.show()\n            \n            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,4)).sum()\n            mae_batch = np.mean(np.abs(predictions-target),  axis=(0,1,4)).sum() \n            total_mse += mse_batch\n            total_mae += mae_batch\n            \n            for j in range(out_len):\n                mse_list[i][j] = np.square(predictions[:,j,:,:,:] - target[:,j,:,:,:]).mean()\n                mae_list[i][j] = np.abs(predictions[:,j,:,:,:] - target[:,j,:,:,:]).mean()\n                ssim_list[i][j] = ssim(target[0,j,:,:,0], predictions[0,j,:,:,0], multichannel=False)\n                psnr_list[i][j] = 20 * np.log10(1 / sqrt(mse_list[i][j]))\n                t1 = torch.from_numpy((predictions[:,j,:,:,:] - 0.5) / 0.5).to(configs.device).permute((0, 3, 1, 2))\n                t2 = torch.from_numpy((target[:,j,:,:,:] - 0.5) / 0.5).to(configs.device).permute((0, 3, 1, 2))\n                d = loss_fn.forward(t1, t2)\n                lpips_list[i][j] = d.mean().detach().cpu().numpy() * 100\n                    \n        #model.train()\n        \n    avg_mse_frame = mse_list.mean(axis=0)\n    avg_mae_frame = mae_list.mean(axis=0)\n    avg_ssim_frame = ssim_list.mean(axis=0)\n    avg_psnr_frame = psnr_list.mean(axis=0)\n    avg_lpips_frame = lpips_list.mean(axis=0)\n\n    avg_mse = mse_list.mean()\n    avg_mae = mae_list.mean()\n    avg_ssim = ssim_list.mean()\n    avg_psnr = psnr_list.mean()\n    avg_lpips = lpips_list.mean()\n\n    print('Eval MSE: ', total_mse/len(test_loader))\n    print('Eval MAE: ', total_mae/len(test_loader))\n    \n    print(f'Avg-MSE: {avg_mse}\\nMSE/Frame: {avg_mse_frame}')\n    print(f'Avg-MAE: {avg_mae}\\nMAE/Frame: {avg_mae_frame}')\n    print(f'Avg-SSIM: {avg_ssim}\\nSSIM/Frame: {avg_ssim_frame}')\n    print(f'Avg-PSNR: {avg_psnr}\\nPSNR/Frame: {avg_psnr_frame}')\n    print(f'Avg-LPIPS: {avg_lpips}\\nLPIPS/Frame: {avg_lpips_frame}')\n    \n    return avg_mse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN TEST WRAPPER**","metadata":{}},{"cell_type":"code","source":"def schedule_sampling(eta, itr, channel, batch_size):\n    zeros = np.zeros((batch_size,\n                      args.total_length - args.input_length - 1,\n                      args.img_height // args.patch_size,\n                      args.img_width // args.patch_size,\n                      args.patch_size ** 2 * channel))\n    if not args.scheduled_sampling:\n        return 0.0, zeros\n\n    if itr < args.sampling_stop_iter:\n        eta -= args.sampling_changing_rate\n    else:\n        eta = 0.0\n    #print('eta: ', eta)\n    random_flip = np.random.random_sample(\n        (batch_size, args.total_length - args.input_length - 1))\n    true_token = (random_flip < eta)\n    ones = np.ones((args.img_height // args.patch_size,\n                    args.img_width // args.patch_size,\n                    args.patch_size ** 2 * channel))\n    zeros = np.zeros((args.img_height // args.patch_size,\n                      args.img_width // args.patch_size,\n                      args.patch_size ** 2 * channel))\n    real_input_flag = []\n    for i in range(batch_size):\n        for j in range(args.total_length - args.input_length - 1):\n            if true_token[i, j]:\n                real_input_flag.append(ones)\n            else:\n                real_input_flag.append(zeros)\n    real_input_flag = np.array(real_input_flag)\n    real_input_flag = np.reshape(real_input_flag,\n                                 (batch_size,\n                                  args.total_length - args.input_length - 1,\n                                  args.img_height // args.patch_size,\n                                  args.img_width // args.patch_size,\n                                  args.patch_size ** 2 * channel))\n    return eta, real_input_flag\n\n\ndef train_wrapper(model):\n    begin = 0\n#     handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n#     meminfo_begin = pynvml.nvmlDeviceGetMemoryInfo(handle)\n\n    if args.pretrained_model:\n        model.load(args.pretrained_model)\n        #begin = int(args.pretrained_model.split('-')[-1])\n\n        \n    # DATASET\n    dataset = TimeSeriesDataset(root_dir=configs.data_train_path, n_frames_input=10, n_frames_output=10)\n    \n    # DATA LOADER + SPLIT\n    validation_split = .3\n    shuffle_dataset = True\n    random_seed= 1000\n\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n    train_input_handle = torch.utils.data.DataLoader(dataset, args.batch_size, sampler=train_sampler, num_workers=2, pin_memory=True)\n    val_input_handle = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=valid_sampler)\n\n    losses_l1 = []\n    losses_l2 = []\n    \n    eta = args.sampling_start_value\n    eta -= (begin * args.sampling_changing_rate)\n    itr = begin\n    # real_input_flag = {}\n    for epoch in range(0, args.max_epoches):\n        \n        if epoch == 0:    \n            pass#evaluation_proper(model, val_input_handle, configs, out_len=10)\n        \n        for ims in tqdm(train_input_handle, total=len(train_input_handle)):\n            if itr > args.max_iterations:\n                break\n            batch_size = ims.shape[0]\n            if(configs.verbose):\n                print('IMS shape: ', ims.shape)\n                print('Stuff input to schedule sampling: ', eta, itr, args.img_channel, batch_size)\n            eta, real_input_flag = schedule_sampling(eta, itr, args.img_channel, batch_size)\n            if(configs.verbose):\n                print('Stuff output from schedule sampling: ', eta, real_input_flag.shape)\n\n            l1, l2 = train(model, ims, real_input_flag, args, itr, next(iter(val_input_handle)))\n            losses_l1.append(l1.item())\n            losses_l2.append(l2.item())\n            \n            if itr % configs.plot_interval == 0:\n                fig, ax = plt.subplots(2, 1, figsize=(13, 5))\n                a = ax.flatten()\n                a[0].plot(losses_l1, 'r')\n                a[0].set_title('Loss L1')\n                a[1].plot(losses_l2, 'r')\n                a[1].set_title('Loss L2')\n                plt.show()\n            \n            if itr % args.snapshot_interval == 0 and itr > begin:\n                model.save(itr)\n            itr += 1\n        print(f'Epoch: [{epoch}/{args.max_epoches}]')\n\n    evaluation_proper(model, val_input_handle, configs, out_len=10)\n            \n#             meminfo_end = pynvml.nvmlDeviceGetMemoryInfo(handle)\n#             if(configs.verbose):\n#                 print(\"GPU memory:%dM\" % ((meminfo_end.used - meminfo_begin.used) / (1024 ** 2)))\n\n\ndef test_wrapper(model, val_ds):\n    model.load(args.pretrained_model)\n    test_input_handle = val_ds\n\n    itr = 1\n    for i in range(itr):\n        trainer.test(model, test_input_handle, args, itr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL FACTORY**","metadata":{}},{"cell_type":"code","source":"class Model(object):\n    def __init__(self, configs):\n        self.configs = configs\n        self.patch_height = configs.img_height // configs.patch_size\n        self.patch_width = configs.img_width // configs.patch_size\n        self.patch_channel = configs.img_channel * (configs.patch_size ** 2)\n        self.num_layers = configs.num_layers\n        networks_map = {\n            'mau': RNN,\n        }\n        num_hidden = []\n        for i in range(configs.num_layers):\n            num_hidden.append(configs.num_hidden)\n        self.num_hidden = num_hidden\n        if configs.model_name in networks_map:\n            Network = networks_map[configs.model_name]\n            self.network = Network(self.num_layers, self.num_hidden, configs).to(configs.device)\n        else:\n            raise ValueError('Name of network unknown %s' % configs.model_name)\n\n        self.optimizer = Adam(self.network.parameters(), lr=configs.lr)\n        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=configs.lr_decay)\n        \n        self.MSE_criterion = nn.MSELoss()\n        self.L1_loss = nn.L1Loss()\n\n    def save(self, itr):\n        stats = {'net_param': self.network.state_dict()}\n        checkpoint_path = os.path.join(self.configs.save_dir, 'model.ckpt' + '-' + str(itr))\n        torch.save(stats, checkpoint_path)\n        print(\"save predictive model to %s\" % checkpoint_path)\n\n    def load(self, pm_checkpoint_path):\n        print('load predictive model:', pm_checkpoint_path)\n        stats = torch.load(pm_checkpoint_path, map_location=torch.device(self.configs.device))\n        self.network.load_state_dict(stats['net_param'])\n\n    def train(self, data, mask, itr, val):\n        frames = data\n        self.network.train()\n        val_tensor = torch.FloatTensor(val).to(self.configs.device)\n        frames_tensor = torch.FloatTensor(frames).to(self.configs.device)\n        mask_tensor = torch.FloatTensor(mask).to(self.configs.device)\n\n        if(self.configs.verbose):\n            print('FT', frames_tensor.shape)\n            print('MT', mask_tensor.shape)\n        \n        next_frames = self.network(frames_tensor, mask_tensor)\n        if(self.configs.verbose):\n            print('Next Frames', next_frames.shape)\n\n        ground_truth = frames_tensor\n        if(self.configs.verbose):\n            print('Ground', ground_truth[:, 1:].shape)\n\n            \n        \n        if itr % configs.plot_interval == 0:\n            with torch.no_grad():\n                self.network.eval()\n                x = frames_tensor[0][0:configs.input_length]\n                y = frames_tensor[0][configs.input_length:]\n                g = next_frames[0][configs.input_length-1:]\n                m = mask_tensor[0]\n                fig, ax = plt.subplots(4, configs.input_length, figsize=(25, 10))\n                for i in range(4):\n                    for j in range(configs.input_length):\n                        if i == 0:\n                            ax[i][j].imshow(x[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('T Input')\n                        if i == 1:\n                            if j == configs.input_length-1:\n                                ax[i][j].axis('off')\n                                continue\n                            ax[i][j].imshow(m[j].to('cpu'))\n                            ax[i][j].set_title('T Mask')\n                        if i == 2:\n                            ax[i][j].imshow(y[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('T Ground Truth')\n                        if i == 3:\n                            ax[i][j].imshow(g[j].detach().to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('T Generated')\n                        ax[i][j].axis('off')\n\n\n                x = val_tensor[0][0:configs.input_length]\n                y = val_tensor[0][configs.input_length:]\n                mask = torch.zeros_like(mask_tensor[0]).unsqueeze(0).to(configs.device)\n                next_frameszz = self.network(val_tensor, mask)\n                m = mask[0]\n                g = next_frameszz[0][configs.input_length-1:]\n                fig, ax = plt.subplots(4, configs.input_length, figsize=(25, 10))\n                for i in range(4):\n                    for j in range(configs.input_length):\n                        if i == 0:\n                            ax[i][j].imshow(x[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('V Input')\n                        if i == 1:\n                            if j == configs.input_length-1:\n                                ax[i][j].axis('off')\n                                continue\n                            ax[i][j].imshow(m[j].to('cpu'))\n                            ax[i][j].set_title('V Mask')\n                        if i == 2:\n                            ax[i][j].imshow(y[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('V Ground Truth')\n                        if i == 3:\n                            ax[i][j].imshow(g[j].detach().to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('V Generated')\n                        ax[i][j].axis('off')\n                \n                self.network.train()\n            \n                    \n            \n        batch_size = next_frames.shape[0]\n\n        self.optimizer.zero_grad()\n        loss_l1 = self.L1_loss(next_frames,\n                               ground_truth[:, 1:])\n        loss_l2 = self.MSE_criterion(next_frames,\n                                     ground_truth[:, 1:])\n        loss_gen = loss_l2\n        loss_gen.backward()\n        self.optimizer.step()\n\n        if itr >= self.configs.sampling_stop_iter and itr % self.configs.delay_interval == 0:\n            self.scheduler.step()\n            # self.scheduler_F.step()\n            # self.scheduler_D.step()\n            print('LR decay to:%.8f', self.optimizer.param_groups[0]['lr'])\n        return next_frames, loss_l1.detach().cpu().numpy(), loss_l2.detach().cpu().numpy()\n\n    def test(self, data, mask):\n        frames = data\n        self.network.eval()\n        frames_tensor = torch.FloatTensor(frames).to(self.configs.device)\n        mask_tensor = torch.FloatTensor(mask).to(self.configs.device)\n        next_frames = self.network(frames_tensor, mask_tensor)\n        return next_frames.detach().cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CONFIG**","metadata":{}},{"cell_type":"code","source":"class Configuration:\n    def __init__(self):\n        super(Configuration, self).__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.data_train_path = '../input/moving-mnist/mnist_test_seq.npy'\n        self.data_test_path = '../input/kthextract-to-jpg/data/test'\n        self.input_length = 10\n        self.real_length = 20\n        self.total_length = 20\n        self.img_height = 64\n        self.img_width = 64\n        self.sr_size = 4\n        self.img_channel = 1\n        self.patch_size = 1\n        self.alpha = 1\n        self.model_name = 'mau'\n        self.dataset = 'mmnist'\n        self.cell_mode = 'normal'\n        self.model_mode = 'recall'\n        self.num_workers = 2\n        self.num_hidden = 64\n        self.num_layers = 4\n        self.num_heads = 4\n        self.filter_size = (5, 5)\n        self.stride = 1\n        self.time = 2\n        self.time_stride = 1\n        self.tau = 5\n        self.is_training = True\n        self.lr = 1e-3\n        self.lr_decay = 0.90\n        self.delay_interval = 2000\n        self.batch_size = 32\n        self.max_iterations = 150000\n        self.max_epoches = 80\n        self.display_interval = 100\n        self.plot_interval = 100\n        self.test_interval = 1010\n        self.snapshot_interval = 1000\n        self.num_save_samples = 3\n        self.n_gpu = 1\n        self.pretrained_model = ''\n        self.perforamnce_dir = 'results/mmnist'\n        self.save_dir = 'saves/mmnist'\n        self.gen_frm_dir = 'results/mmnist/'\n        self.scheduled_sampling = True\n        self.sampling_stop_iter = 50000\n        self.sampling_start_value = 1.0\n        self.sampling_changing_rate = 0.000005\n        self.verbose = False\n        \nconfigs = Configuration()\nargs = configs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST CASE","metadata":{}},{"cell_type":"code","source":"def test_model(configs):\n    nl = 4\n    nh = [64, 64, 64, 64]\n    z = torch.randn(1, 20, 1, 256, 256).to(configs.device)\n    m = torch.zeros(1, 9, 256, 256, 1).to(configs.device)\n    model = RNN(nl, nh, configs).to(configs.device)\n    g = model(z, m, True)\n    print(g.shape)\n\n#test_model(configs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = Model(args)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count_parameters(model.network)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN**","metadata":{}},{"cell_type":"code","source":"#pynvml.nvmlInit()\n\nprint('Initializing models')\n\nmodel = Model(args)\n\nif args.is_training:\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n    if not os.path.exists(args.gen_frm_dir):\n        os.makedirs(args.gen_frm_dir)\n    train_wrapper(model)\nelse:\n    if not os.path.exists(args.gen_frm_dir):\n        os.makedirs(args.gen_frm_dir)\n    test_wrapper(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}