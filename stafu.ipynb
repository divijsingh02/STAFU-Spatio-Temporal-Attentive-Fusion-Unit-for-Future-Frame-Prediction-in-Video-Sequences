{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **IMPORTS**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.utils.data as data\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport numpy as np\nfrom tqdm import tqdm\nfrom numpy import *\nfrom numpy.linalg import *\nfrom scipy.special import factorial\nfrom functools import reduce\nimport random\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport time\nimport gzip\nimport cv2\nimport math\nimport os\nfrom PIL import Image\nfrom skimage.metrics import structural_similarity as ssim\nfrom skimage.transform import resize\nimport argparse\n!pip install lpips\nimport codecs\nimport lpips","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:09.872662Z","iopub.execute_input":"2022-06-20T07:19:09.873392Z","iopub.status.idle":"2022-06-20T07:19:17.25904Z","shell.execute_reply.started":"2022-06-20T07:19:09.873353Z","shell.execute_reply":"2022-06-20T07:19:17.258121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **UTILITY**","metadata":{}},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.265214Z","iopub.execute_input":"2022-06-20T07:19:17.267651Z","iopub.status.idle":"2022-06-20T07:19:17.274591Z","shell.execute_reply.started":"2022-06-20T07:19:17.267521Z","shell.execute_reply":"2022-06-20T07:19:17.27392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def schedule_sampling(eta, itr, channel, batch_size):\n    zeros = np.zeros((batch_size, args.total_length - args.input_length - 1, args.img_height, args.img_width, channel))\n    if not args.scheduled_sampling:\n        return 0.0, zeros\n\n    if itr < args.sampling_stop_iter:\n        eta -= args.sampling_changing_rate\n    else:\n        eta = 0.0\n        \n    if(configs.verbose or itr % 100 == 0):\n        print('ETA: ', eta)\n    random_flip = np.random.random_sample((batch_size, args.total_length - args.input_length - 1))\n    true_token = (random_flip < eta) # replace 0.5 with eta\n    ones = np.ones((args.img_height, args.img_width, channel))\n    zeros = np.zeros((args.img_height, args.img_width, channel))\n    \n    mask = []\n    for i in range(batch_size):\n        for j in range(args.total_length - args.input_length - 1):\n            if true_token[i, j]:\n                mask.append(ones)\n            else:\n                mask.append(zeros)\n                \n    mask = np.array(mask)\n    mask = np.reshape(mask, (batch_size, args.total_length - args.input_length - 1, args.img_height, args.img_width, channel))\n    return eta, mask","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.279715Z","iopub.execute_input":"2022-06-20T07:19:17.281448Z","iopub.status.idle":"2022-06-20T07:19:17.29959Z","shell.execute_reply.started":"2022-06-20T07:19:17.281396Z","shell.execute_reply":"2022-06-20T07:19:17.298784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Resize2(object):\n\n    def __call__(self, sample):\n        imgs_out = np.zeros((\n            sample.shape[0], 640, 480, sample.shape[3]))\n        for i in range(sample.shape[0]):\n            imgs_out[i,:,:,:] = resize(sample[i,:,:,:], imgs_out.shape[1:])\n        return imgs_out","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.306285Z","iopub.execute_input":"2022-06-20T07:19:17.309245Z","iopub.status.idle":"2022-06-20T07:19:17.318908Z","shell.execute_reply.started":"2022-06-20T07:19:17.309205Z","shell.execute_reply":"2022-06-20T07:19:17.31802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation_proper(model, test_loader, configs, out_len=10):\n    print('Evaluating...')\n    \n    loss_fn = lpips.LPIPS(net='alex', spatial=True).to(configs.device)\n    mse_list = np.empty((len(test_loader), out_len))\n    mae_list = np.empty((len(test_loader), out_len))\n    ssim_list = np.empty((len(test_loader), out_len))\n    psnr_list = np.empty((len(test_loader), out_len))\n    lpips_list = np.empty((len(test_loader), out_len))\n    \n    total_mse = 0\n    total_mae = 0\n    \n    with torch.no_grad():\n        #model.eval()\n        for i, data in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n#             if i == 1000:\n#                 break\n            batch_size = data.shape[0]\n            real_input_flag = np.zeros(\n                (batch_size,\n                 configs.total_length - configs.input_length - 1,\n                 configs.img_height // configs.patch_size,\n                 configs.img_width // configs.patch_size,\n                 configs.patch_size ** 2 * configs.img_channel))\n\n            img_gen = model.test(data, real_input_flag)\n            img_gen = img_gen.transpose(0, 1, 3, 4, 2)  # * 0.5 + 0.5\n            test_ims = data.detach().cpu().numpy().transpose(0, 1, 3, 4, 2)  # * 0.5 + 0.5\n            output_length = configs.total_length - configs.input_length\n            output_length = min(output_length, configs.total_length - 1)\n            target = data[:, configs.input_length:, :].detach().cpu().numpy().transpose(0, 1, 3, 4, 2)\n            predictions = img_gen[:, -output_length:, :]\n            \n            p_min = predictions.min()\n            p_max = predictions.max()\n            n_min = 0\n            n_max = 1\n            \n            #predictions = (predictions - p_min)/(p_max - p_min)*(n_max - n_min) + n_min\n            predictions[predictions < 0.10] = 0\n            predictions[predictions > 0.99] = 1\n            \n            if (i+1) % 50 == 0:\n                print(target[0, 1, 40:42, 40:42, 0])\n                print(predictions[0, 1, 40:42, 40:42, 0])\n                fig, ax = plt.subplots(2, out_len, figsize=(25, 7))\n                for i in range(2):\n                    for j in range(out_len):\n                        if i == 0:\n                            ax[i][j].imshow(target[0][j])\n                            ax[i][j].set_title('V Ground Truth')\n                        if i == 1:\n                            ax[i][j].imshow(predictions[0][j])\n                            ax[i][j].set_title('V Generated')\n                        ax[i][j].axis('off')\n                plt.show()\n            \n            mse_batch = np.mean((predictions-target)**2 , axis=(0,1,4)).sum()\n            mae_batch = np.mean(np.abs(predictions-target),  axis=(0,1,4)).sum() \n            total_mse += mse_batch\n            total_mae += mae_batch\n            \n            for j in range(out_len):\n                mse_list[i][j] = np.square(predictions[:,j,:,:,:] - target[:,j,:,:,:]).mean()\n                mae_list[i][j] = np.abs(predictions[:,j,:,:,:] - target[:,j,:,:,:]).mean()\n                ssim_list[i][j] = ssim(target[0,j,:,:,:], predictions[0,j,:,:,:], multichannel=True)\n                psnr_list[i][j] = 20 * np.log10(1 / sqrt(mse_list[i][j]))\n                t1 = torch.from_numpy((predictions[:,j,:,:,:] - 0.5) / 0.5).to(configs.device).permute((0, 3, 1, 2))\n                t2 = torch.from_numpy((target[:,j,:,:,:] - 0.5) / 0.5).to(configs.device).permute((0, 3, 1, 2))\n                d = loss_fn.forward(t1, t2)\n                lpips_list[i][j] = d.mean().detach().cpu().numpy() * 100\n                    \n        #model.train()\n        \n    avg_mse_frame = mse_list.mean(axis=0)\n    avg_mae_frame = mae_list.mean(axis=0)\n    avg_ssim_frame = ssim_list.mean(axis=0)\n    avg_psnr_frame = psnr_list.mean(axis=0)\n    avg_lpips_frame = lpips_list.mean(axis=0)\n\n    avg_mse = mse_list.mean()\n    avg_mae = mae_list.mean()\n    avg_ssim = ssim_list.mean()\n    avg_psnr = psnr_list.mean()\n    avg_lpips = lpips_list.mean()\n\n    print('Eval MSE: ', total_mse/len(test_loader))\n    print('Eval MAE: ', total_mae/len(test_loader))\n    \n    print(f'Avg-MSE: {avg_mse}\\nMSE/Frame: {avg_mse_frame}')\n    print(f'Avg-MAE: {avg_mae}\\nMAE/Frame: {avg_mae_frame}')\n    print(f'Avg-SSIM: {avg_ssim}\\nSSIM/Frame: {avg_ssim_frame}')\n    print(f'Avg-PSNR: {avg_psnr}\\nPSNR/Frame: {avg_psnr_frame}')\n    print(f'Avg-LPIPS: {avg_lpips}\\nLPIPS/Frame: {avg_lpips_frame}')\n    \n    return avg_mse","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.324777Z","iopub.execute_input":"2022-06-20T07:19:17.327478Z","iopub.status.idle":"2022-06-20T07:19:17.373927Z","shell.execute_reply.started":"2022-06-20T07:19:17.327431Z","shell.execute_reply":"2022-06-20T07:19:17.372872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DATA LOADER**","metadata":{}},{"cell_type":"code","source":"class Norm(object):\n    def __init__(self, max=255):\n        self.max = max\n\n    def __call__(self, sample):\n        video_x = sample\n        new_video_x = video_x / self.max\n        return new_video_x\n\n\nclass ToTensor(object):\n\n    def __call__(self, sample):\n        video_x = sample\n        video_x = video_x.transpose((0, 3, 1, 2))\n        video_x = np.array(video_x)\n        return torch.from_numpy(video_x).float()\n    \n\nclass Resize(object):\n\n    def __call__(self, sample):\n        imgs_out = np.zeros((\n            sample.shape[0], configs.img_height, configs.img_width, sample.shape[3]))\n        for i in range(sample.shape[0]):\n            imgs_out[i,:,:,:] = resize(sample[i,:,:,:], imgs_out.shape[1:])\n        return imgs_out\n    \nclass Resize2(object):\n\n    def __call__(self, sample):\n        imgs_out = np.zeros((\n            sample.shape[0], 640, 480, sample.shape[3]))\n        for i in range(sample.shape[0]):\n            imgs_out[i,:,:,:] = resize(sample[i,:,:,:], imgs_out.shape[1:])\n        return imgs_out","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.377025Z","iopub.execute_input":"2022-06-20T07:19:17.377378Z","iopub.status.idle":"2022-06-20T07:19:17.39383Z","shell.execute_reply.started":"2022-06-20T07:19:17.37734Z","shell.execute_reply":"2022-06-20T07:19:17.392647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesDatasetNpz(data.Dataset):\n    def __init__(self, root_dir, n_frames_input=10, n_frames_output=10):\n        self.n_frames_in = n_frames_input\n        self.n_frames_out = n_frames_output\n        n_frames = n_frames_input + n_frames_output\n        \n        self.file = np.load(root_dir).transpose(1,0,2,3)[..., np.newaxis].transpose(0,1,4,2,3)\n        #self.file = np.load(root_dir).transpose(1,0,4,2,3)\n            \n            \n    def __len__(self):\n        return len(self.file)\n\n    def __getitem__(self, index):\n        clips = torch.from_numpy(self.file[index])\n        clips = clips.type(torch.float32)\n        clips = (clips / 255)\n        return clips","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.395826Z","iopub.execute_input":"2022-06-20T07:19:17.396701Z","iopub.status.idle":"2022-06-20T07:19:17.405955Z","shell.execute_reply.started":"2022-06-20T07:19:17.39663Z","shell.execute_reply":"2022-06-20T07:19:17.405164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeSeriesDataset(data.Dataset):\n    def __init__(self, root_dir, n_frames_input=10, n_frames_output=10):\n        view_type='090'\n        random.seed(1000)\n        self.n_frames_in = n_frames_input\n        self.n_frames_out = n_frames_output\n        n_frames = n_frames_input + n_frames_output\n        subject_dirs = [d for d in os.listdir(root_dir)] # [01, 02, 03]\n        subject_dirs = [os.path.join(root_dir, subject_dirs[i]) for i in range(len(subject_dirs))] # root/01/01/\n        seq_type_dirs = [os.path.join(d, sd) for d in subject_dirs for sd in os.listdir(d)] # [root/01/01/bg1, root/01/01/bg2]\n        view_type_dirs = sorted([[os.path.join(d, sd)] for d in seq_type_dirs for sd in os.listdir(d) if sd == view_type])\n\n        self.specific_view_files = []\n        print(len(view_type_dirs))\n        \n        for d in view_type_dirs:\n            self.specific_view_files.append(sorted([os.path.join(d[0], f) for f in os.listdir(d[0])]))\n        \n        self.nframes_list = []\n        for f in self.specific_view_files[:50]:\n            for i in range(len(f)-n_frames):\n                self.nframes_list.append(f[i:i+n_frames])\n        \n        self.trans_norm = transforms.Compose([Norm()])\n        self.trans_tensor = transforms.Compose([ToTensor()])\n        self.trans_resize = transforms.Compose([Resize()])\n        \n        print(len(self.nframes_list))\n            \n            \n    def __len__(self):\n        return len(self.nframes_list)\n\n    def __getitem__(self, index):\n        data_seq = np.ndarray(shape=(len(self.nframes_list[index]), 64, 64), dtype=np.uint8)\n        \n        for i, f in enumerate(self.nframes_list[index]):\n            data_seq[i, :] = cv2.resize(np.array(Image.open(f)), (64, 64))\n            \n        data_seq = data_seq[..., np.newaxis]\n        input = self.trans_norm(data_seq)  \n        #input = self.trans_resize(input)\n        #input = reshape_patch(input, configs.patch_size)\n        input = self.trans_tensor(input)\n        return input","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.407616Z","iopub.execute_input":"2022-06-20T07:19:17.408378Z","iopub.status.idle":"2022-06-20T07:19:17.432559Z","shell.execute_reply.started":"2022-06-20T07:19:17.408308Z","shell.execute_reply":"2022-06-20T07:19:17.431961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST CASE","metadata":{}},{"cell_type":"code","source":"# td = TimeSeriesDataset(root_dir='../../input/ethzzz/dataset_ETHZ/seq1', n_frames_input=10, n_frames_output=10)\n# train_loader = torch.utils.data.DataLoader(dataset=td, batch_size=3, shuffle=True, num_workers=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.43418Z","iopub.execute_input":"2022-06-20T07:19:17.434792Z","iopub.status.idle":"2022-06-20T07:19:17.445772Z","shell.execute_reply.started":"2022-06-20T07:19:17.434752Z","shell.execute_reply":"2022-06-20T07:19:17.444908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# z = next(iter(train_loader))\n# print(z.shape)\n# print(torch.max(z), torch.min(z))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.451338Z","iopub.execute_input":"2022-06-20T07:19:17.452239Z","iopub.status.idle":"2022-06-20T07:19:17.456488Z","shell.execute_reply.started":"2022-06-20T07:19:17.452194Z","shell.execute_reply":"2022-06-20T07:19:17.455667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure()\n# plt.imshow(z[0][0].permute(1,2,0)) \n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.458393Z","iopub.execute_input":"2022-06-20T07:19:17.459179Z","iopub.status.idle":"2022-06-20T07:19:17.463635Z","shell.execute_reply.started":"2022-06-20T07:19:17.459111Z","shell.execute_reply":"2022-06-20T07:19:17.462771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DEPTH**","metadata":{}},{"cell_type":"code","source":"# !pip install gdown","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.46547Z","iopub.execute_input":"2022-06-20T07:19:17.468272Z","iopub.status.idle":"2022-06-20T07:19:17.473168Z","shell.execute_reply.started":"2022-06-20T07:19:17.468233Z","shell.execute_reply":"2022-06-20T07:19:17.472243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gdown\n\n# !git clone 'https://github.com/shariqfarooq123/AdaBins'","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.476182Z","iopub.execute_input":"2022-06-20T07:19:17.477566Z","iopub.status.idle":"2022-06-20T07:19:17.481872Z","shell.execute_reply.started":"2022-06-20T07:19:17.477526Z","shell.execute_reply":"2022-06-20T07:19:17.480938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd AdaBins","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.4834Z","iopub.execute_input":"2022-06-20T07:19:17.48426Z","iopub.status.idle":"2022-06-20T07:19:17.499002Z","shell.execute_reply.started":"2022-06-20T07:19:17.484221Z","shell.execute_reply":"2022-06-20T07:19:17.49588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !gdown https://drive.google.com/uc?id=1lvyZZbC9NLcS8a__YPcUP7rDiIpbRpoF\n# !mkdir pretrained\n# !mv AdaBins_nyu.pt pretrained/AdaBins_nyu.pt","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.502014Z","iopub.execute_input":"2022-06-20T07:19:17.504806Z","iopub.status.idle":"2022-06-20T07:19:17.512478Z","shell.execute_reply.started":"2022-06-20T07:19:17.504767Z","shell.execute_reply":"2022-06-20T07:19:17.511534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from infer import InferenceHelper\n\n# infer_helper = InferenceHelper(dataset='nyu')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.516542Z","iopub.execute_input":"2022-06-20T07:19:17.517661Z","iopub.status.idle":"2022-06-20T07:19:17.522902Z","shell.execute_reply.started":"2022-06-20T07:19:17.517622Z","shell.execute_reply":"2022-06-20T07:19:17.521983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [os.path.join('../../input/ethzzz/dataset_ETHZ/seq1', d) for d in os.listdir('../../input/ethzzz/dataset_ETHZ/seq1')][:5]","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.526065Z","iopub.execute_input":"2022-06-20T07:19:17.527505Z","iopub.status.idle":"2022-06-20T07:19:17.533565Z","shell.execute_reply.started":"2022-06-20T07:19:17.527468Z","shell.execute_reply":"2022-06-20T07:19:17.532521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir output_depth","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.536962Z","iopub.execute_input":"2022-06-20T07:19:17.538037Z","iopub.status.idle":"2022-06-20T07:19:17.543384Z","shell.execute_reply.started":"2022-06-20T07:19:17.537999Z","shell.execute_reply":"2022-06-20T07:19:17.542535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#infer_helper.predict_dir(\"../../input/ethzzz/dataset_ETHZ/seq1/p001/\", \"../output_depth\")","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.546458Z","iopub.execute_input":"2022-06-20T07:19:17.547209Z","iopub.status.idle":"2022-06-20T07:19:17.553881Z","shell.execute_reply.started":"2022-06-20T07:19:17.547169Z","shell.execute_reply":"2022-06-20T07:19:17.553144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example_rgb_batch = next(iter(train_loader)).to(configs.device)\n# print(example_rgb_batch[:,0,:].shape)\n# bin_centers, predicted_depth = infer_helper.predict(example_rgb_batch[:,0,:])","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.556518Z","iopub.execute_input":"2022-06-20T07:19:17.557014Z","iopub.status.idle":"2022-06-20T07:19:17.56229Z","shell.execute_reply.started":"2022-06-20T07:19:17.556978Z","shell.execute_reply":"2022-06-20T07:19:17.56162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(example_rgb_batch[0,0,:].cpu().permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.56575Z","iopub.execute_input":"2022-06-20T07:19:17.566356Z","iopub.status.idle":"2022-06-20T07:19:17.572252Z","shell.execute_reply.started":"2022-06-20T07:19:17.566314Z","shell.execute_reply":"2022-06-20T07:19:17.571439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(predicted_depth[0][0], cmap='plasma')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.575143Z","iopub.execute_input":"2022-06-20T07:19:17.577021Z","iopub.status.idle":"2022-06-20T07:19:17.582661Z","shell.execute_reply.started":"2022-06-20T07:19:17.576983Z","shell.execute_reply":"2022-06-20T07:19:17.581652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODELS**","metadata":{}},{"cell_type":"markdown","source":"## MAU-CELL","metadata":{}},{"cell_type":"code","source":"ix = 0","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.584573Z","iopub.execute_input":"2022-06-20T07:19:17.585356Z","iopub.status.idle":"2022-06-20T07:19:17.589756Z","shell.execute_reply.started":"2022-06-20T07:19:17.585301Z","shell.execute_reply":"2022-06-20T07:19:17.589038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MAUCell(nn.Module):\n    def __init__(self, in_channel, num_hidden, height, width, filter_size, stride, tau, cell_mode):\n        super(MAUCell, self).__init__()\n        \n        self.num_hidden = num_hidden\n        self.padding = (filter_size[0] // 2, filter_size[1] // 2)\n        self.cell_mode = cell_mode\n        self.d = num_hidden * height * width\n        self.tau = tau\n        self.states = ['residual', 'normal']\n        if not self.cell_mode in self.states:\n            raise AssertionError\n        self.conv_t = nn.Sequential(\n            nn.Conv2d(in_channel, 4 * num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding),\n            nn.LayerNorm([4 * num_hidden, height, width])\n        )\n        self.conv_t_next = nn.Sequential(\n            nn.Conv2d(in_channel, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding),\n            nn.LayerNorm([num_hidden, height, width])\n        )\n        \n        self.conv_s = nn.Sequential(\n            nn.Conv2d(num_hidden, 4 * num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding),\n            nn.LayerNorm([4 * num_hidden, height, width])\n        )\n        \n        self.conv_s_next = nn.Sequential(\n            nn.Conv2d(num_hidden, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding),\n            nn.LayerNorm([num_hidden, height, width])\n        )\n        \n        self.conv_t_i = nn.Sequential(\n            nn.Conv2d(num_hidden, num_hidden, kernel_size=filter_size, stride=stride, padding=self.padding),\n            nn.LayerNorm([num_hidden, height, width])\n        )\n        \n        self.alpha_s = nn.Parameter(torch.randn(1))\n        self.alpha_t = nn.Parameter(torch.randn(1))\n        \n        self.softmax = nn.Softmax(dim=0)\n\n        \n    def forward(self, T_t, S_t, t_att, s_att, s_pixel_att):\n        global ix\n        s_next = self.conv_s_next(S_t)\n        t_next = self.conv_t_next(T_t)\n\n        weights_list = []\n        for i in range(self.tau):\n            weights_list.append((s_att[i] * s_next).sum(dim=(1, 2, 3)) / math.sqrt(self.d))\n        weights_list = torch.stack(weights_list, dim=0)\n        weights_list = torch.reshape(weights_list, (*weights_list.shape, 1, 1, 1))\n        weights_list = self.softmax(weights_list)\n\n        T_trend = t_att * weights_list\n        T_trend = T_trend.sum(dim=0)\n        t_att_gate = torch.sigmoid(t_next)\n        s_att_gate = torch.sigmoid(s_next)\n        \n        T_fusion = T_t * t_att_gate + (1 - t_att_gate) * T_trend\n        #S_fusion = S_t * s_att_gate + (1 - s_att_gate) * torch.sigmoid(s_pixel_att) * T_trend\n        #S_fusion = S_t * s_att_gate + (1 - s_att_gate) * s_pixel_att\n        S_fusion = S_t * torch.sigmoid(s_pixel_att)\n        \n        T_concat = self.conv_t(T_fusion)\n        S_concat = self.conv_s(S_fusion)\n\n        t_i, t_r, t_t, t_s = torch.split(T_concat, self.num_hidden, dim=1)\n        s_i, s_r, s_t, s_s = torch.split(S_concat, self.num_hidden, dim=1)\n\n#         T_c = torch.tanh(T_fusion*t_r)\n#         S_c = torch.tanh(S_fusion*s_r)\n        \n        T_i = torch.tanh(t_i)\n        S_i = torch.tanh(s_i)\n        \n        T_r = torch.sigmoid(t_r)\n        S_r = torch.sigmoid(t_r)\n        \n        T_t = torch.sigmoid(t_t)\n        S_s = torch.sigmoid(s_s)\n        \n        T_s = torch.sigmoid(t_s)\n        S_t = torch.sigmoid(s_t)\n\n        T_new_1 = T_r * T_i + S_t * T_fusion\n        S_new_1 = S_r * S_i + T_s * S_fusion\n\n        T_new_2 = T_r * t_i + (1 - T_r) * s_t\n        S_new_2 = S_r * s_i + (1 - S_r) * t_s\n        \n        if ix % 9000 == 0:\n            print(self.alpha_s.item(), self.alpha_t.item())\n                \n        out_S = self.alpha_s*S_new_1 + (1-self.alpha_s)*S_new_2\n        out_T = self.alpha_t*T_new_1 + (1-self.alpha_t)*T_new_2\n\n        #if self.cell_mode == 'residual':\n           # S_new = S_t + S_new\n        ix += 1\n        return out_T, out_S\n        #return T_new_2, S_new_2\n        #return T_new_1, S_new_1\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.591376Z","iopub.execute_input":"2022-06-20T07:19:17.591975Z","iopub.status.idle":"2022-06-20T07:19:17.633825Z","shell.execute_reply.started":"2022-06-20T07:19:17.591932Z","shell.execute_reply":"2022-06-20T07:19:17.632327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MAU","metadata":{}},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, num_layers, num_hidden, configs):\n        super(RNN, self).__init__()\n        # print(configs.srcnn_tf)\n        self.configs = configs\n        self.frame_channel = configs.img_channel\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.tau = configs.tau\n        self.cell_mode = configs.cell_mode\n        self.states = ['recall', 'normal']\n        if not self.configs.model_mode in self.states:\n            raise AssertionError\n        # self.time = 2\n        cell_list = []\n\n        width = configs.img_width // configs.sr_size\n        height = configs.img_height // configs.sr_size\n        # print(width)\n\n        for i in range(num_layers):\n            in_channel = num_hidden[i - 1]\n            cell_list.append(\n                MAUCell(in_channel, num_hidden[i], height, width, configs.filter_size,\n                        configs.stride, self.tau, self.cell_mode)\n            )\n        self.cell_list = nn.ModuleList(cell_list)\n\n        # Encoder\n        n = int(math.log2(configs.sr_size))\n        encoders = []\n        encoder = nn.Sequential()\n        encoder.add_module(name='encoder_t_conv{0}'.format(-1),\n                           module=nn.Conv2d(in_channels=self.frame_channel,\n                                            out_channels=self.num_hidden[0],\n                                            stride=1,\n                                            padding=0,\n                                            kernel_size=1))\n        encoder.add_module(name='relu_t_{0}'.format(-1),\n                           module=nn.LeakyReLU(0.2))\n        encoders.append(encoder)\n        for i in range(n):\n            encoder = nn.Sequential()\n            encoder.add_module(name='encoder_t{0}'.format(i),\n                               module=nn.Conv2d(in_channels=self.num_hidden[0],\n                                                out_channels=self.num_hidden[0],\n                                                stride=(2, 2),\n                                                padding=(1, 1),\n                                                kernel_size=(3, 3)\n                                                ))\n            # self.encoder_t.add_module(name='gn_t{0}'.format(i),\n            #                           module=nn.GroupNorm(4, self.frame_channel))\n            encoder.add_module(name='encoder_t_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            encoders.append(encoder)\n        self.encoders = nn.ModuleList(encoders)\n\n        # Decoder\n        decoders = []\n\n        for i in range(n - 1):\n            decoder = nn.Sequential()\n            decoder.add_module(name='c_decoder{0}'.format(i),\n                               module=nn.ConvTranspose2d(in_channels=self.num_hidden[-1],\n                                                         out_channels=self.num_hidden[-1],\n                                                         stride=(2, 2),\n                                                         padding=(1, 1),\n                                                         kernel_size=(3, 3),\n                                                         output_padding=(1, 1)\n                                                         ))\n            # self.decoder_s.add_module(name='gn_decoder_s{0}'.format(i),\n            #                           module=nn.GroupNorm(4, self.frame_channel))\n            decoder.add_module(name='c_decoder_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            decoders.append(decoder)\n\n        if n > 0:\n            decoder = nn.Sequential()\n            decoder.add_module(name='c_decoder{0}'.format(n - 1),\n                               module=nn.ConvTranspose2d(in_channels=self.num_hidden[-1],\n                                                         out_channels=self.num_hidden[-1],\n                                                         stride=(2, 2),\n                                                         padding=(1, 1),\n                                                         kernel_size=(3, 3),\n                                                         output_padding=(1, 1)\n                                                         ))\n            decoders.append(decoder)\n        self.decoders = nn.ModuleList(decoders)\n\n        self.srcnn = nn.Sequential(\n            nn.Conv2d(self.num_hidden[-1], self.frame_channel, kernel_size=1, stride=1, padding=0)\n        )\n        self.merge = nn.Conv2d(self.num_hidden[-1] * 2, self.num_hidden[-1], kernel_size=1, stride=1, padding=0)\n        self.conv_last_sr = nn.Conv2d(self.frame_channel * 2, self.frame_channel, kernel_size=1, stride=1, padding=0)\n\n\n    def forward(self, frames, mask_true, verbose=False):\n        # print('ok')\n        mask_true = mask_true.permute(0, 1, 4, 2, 3).contiguous()\n        if(verbose):\n            print('MT Permuted to: ',  mask_true.shape)\n        batch_size = frames.shape[0]\n        height = frames.shape[3] // self.configs.sr_size\n        width = frames.shape[4] // self.configs.sr_size\n        frame_channels = frames.shape[2]\n        next_frames = []\n        T_t = []\n        T_pre = []\n        S_pre = []\n        # H_t = []\n        x_gen = None\n        if(verbose):\n            print('Num Layers: ', self.num_layers)\n            print('Num Hidden: ', self.num_hidden)\n            print('TAU: ', self.tau)\n        for layer_idx in range(self.num_layers):\n            tmp_t = []\n            tmp_s = []\n            if layer_idx == 0:\n                in_channel = self.num_hidden[layer_idx]\n            else:\n                in_channel = self.num_hidden[layer_idx - 1]\n            for i in range(self.tau):\n                if(verbose):\n                    if i==2:\n                        print('tmp_t[1]', tmp_t[1].shape)\n                tmp_t.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n                tmp_s.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n            T_pre.append(tmp_t)\n            S_pre.append(tmp_s)\n\n        if(verbose):\n            print('len T_pre', len(T_pre))\n            print('len T_pre[1]', len(T_pre[1]))\n\n        S_t_previ = torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device)\n            \n        for t in range(self.configs.total_length - 1):\n            if t < self.configs.input_length:\n                net = frames[:, t]\n                if(verbose):\n                    print('Net frames[:, t]', frames[:, t].shape)\n            else:\n                time_diff = t - self.configs.input_length\n                net = mask_true[:, time_diff] * frames[:, t] + (1 - mask_true[:, time_diff]) * x_gen\n                if(verbose):\n                    print('Net', net.shape)\n\n            frames_feature = net\n            frames_feature_encoded = []\n            if(verbose):\n                print('Len Encoders', len(self.encoders))\n            for i in range(len(self.encoders)):\n                frames_feature = self.encoders[i](frames_feature)\n                if(verbose):\n                    print('Frames_feature', i, frames_feature.shape)\n                frames_feature_encoded.append(frames_feature)\n            if t == 0:\n                for i in range(self.num_layers):\n                    zeros = torch.zeros([batch_size, self.num_hidden[i], height, width]).to(self.configs.device)\n                    T_t.append(zeros)\n                    # print('ok')\n            S_t = frames_feature\n            if(verbose):\n                print('S_t in', S_t.shape)\n            \n            S_pixel_att = torch.sum((S_t - S_t_previ)**2)\n            S_t_previ = S_t\n            \n                \n            for i in range(self.num_layers):\n                t_att = T_pre[i][-self.tau:]\n                t_att = torch.stack(t_att, dim=0)\n                s_att = S_pre[i][-self.tau:]\n                s_att = torch.stack(s_att, dim=0)\n                S_pre[i].append(S_t)\n                if i < 2 and verbose:\n                    print('T_t', len(T_t))\n                    print('T_t[i], S_t, t_att, s_att', T_t[i].shape, S_t.shape, t_att.shape, s_att.shape)\n                T_t[i], S_t = self.cell_list[i](T_t[i], S_t, t_att, s_att, S_pixel_att)\n                T_pre[i].append(T_t[i])\n                # S_pre[i].append(S_t)\n            out = S_t\n            if(verbose):\n                print('S_t out', S_t.shape)\n            # out = self.merge(torch.cat([T_t[-1], S_t], dim=1))\n            frames_feature_decoded = []\n            for i in range(len(self.decoders)):\n                out = self.decoders[i](out)\n                if(verbose):\n                    print('S_t out', i, out.shape)\n                # print(\"ok\")\n                if self.configs.model_mode == 'recall':\n                    # print('unet')\n                    out = out + frames_feature_encoded[-2 - i]\n                    if(verbose):\n                        print('S_t out unet', i, out.shape)\n            # out = self.decoder(out)\n\n            x_gen = self.srcnn(out)\n            next_frames.append(x_gen)\n            if(verbose):\n                print('x_gen', x_gen.shape)\n                print('len next_frames', len(next_frames))\n        if(verbose):\n            print('len next_frames FULL', len(next_frames))\n        next_frames = torch.stack(next_frames, dim=0)\n        if(verbose):\n            print('next_frames Tensor', next_frames.shape)\n        next_frames = next_frames.permute(1, 0, 2, 3, 4).contiguous()\n        if(verbose):\n            print('next_frames Tensor Permuted', next_frames.shape)\n        return next_frames","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.638119Z","iopub.execute_input":"2022-06-20T07:19:17.64071Z","iopub.status.idle":"2022-06-20T07:19:17.753582Z","shell.execute_reply.started":"2022-06-20T07:19:17.640671Z","shell.execute_reply":"2022-06-20T07:19:17.752792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DISCRIMINATOR","metadata":{}},{"cell_type":"code","source":"class FDU(nn.Module):\n    def __init__(self, num_layers, num_hidden, configs):\n        super(FDU, self).__init__()\n\n        self.configs = configs\n        self.frame_channel = configs.img_channel\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.tau = configs.tau\n        self.cell_mode = configs.cell_mode\n\n        cell_list = []\n\n        width = configs.img_width // configs.sr_size\n        height = configs.img_height // configs.sr_size\n\n        for i in range(num_layers):\n            in_channel = num_hidden[i - 1]\n            cell_list.append(\n                MAUCell(in_channel, num_hidden[i], height, width, configs.filter_size,\n                        configs.stride, self.tau, self.cell_mode)\n            )\n        self.cell_list = nn.ModuleList(cell_list)\n\n        # Encoder\n        n = int(math.log2(configs.sr_size))\n        encoders = []\n        encoder = nn.Sequential()\n        encoder.add_module(name='encoder_t_conv{0}'.format(-1),\n                           module=nn.Conv2d(in_channels=self.frame_channel,\n                                            out_channels=self.num_hidden[0],\n                                            stride=1,\n                                            padding=0,\n                                            kernel_size=1))\n        encoder.add_module(name='relu_t_{0}'.format(-1),\n                           module=nn.LeakyReLU(0.2))\n        encoders.append(encoder)\n        for i in range(n):\n            encoder = nn.Sequential()\n            encoder.add_module(name='encoder_t{0}'.format(i),\n                               module=nn.Conv2d(in_channels=self.num_hidden[0],\n                                                out_channels=self.num_hidden[0],\n                                                stride=(2, 2),\n                                                padding=(1, 1),\n                                                kernel_size=(3, 3)\n                                                ))\n            # self.encoder_t.add_module(name='gn_t{0}'.format(i),\n            #                           module=nn.GroupNorm(4, self.frame_channel))\n            encoder.add_module(name='encoder_t_relu{0}'.format(i),\n                               module=nn.LeakyReLU(0.2))\n            encoders.append(encoder)\n        self.encoders = nn.ModuleList(encoders)\n\n\n    def forward(self, frames, verbose=False):\n        if(verbose):\n            print('MT Permuted to: ',  mask_true.shape)\n        batch_size = frames.shape[0]\n        height = frames.shape[3] // self.configs.sr_size\n        width = frames.shape[4] // self.configs.sr_size\n        frame_channels = frames.shape[2]\n        next_frames = []\n        next_memory = []\n        T_t = []\n        T_pre = []\n        S_pre = []\n        # H_t = []\n        x_gen = None\n        if(verbose):\n            print('Num Layers: ', self.num_layers)\n            print('Num Hidden: ', self.num_hidden)\n            print('TAU: ', self.tau)\n        for layer_idx in range(self.num_layers):\n            tmp_t = []\n            tmp_s = []\n            if layer_idx == 0:\n                in_channel = self.num_hidden[layer_idx]\n            else:\n                in_channel = self.num_hidden[layer_idx - 1]\n            for i in range(self.tau):\n                if(verbose):\n                    if i==2:\n                        print('tmp_t[1]', tmp_t[1].shape)\n                tmp_t.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n                tmp_s.append(torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device))\n            T_pre.append(tmp_t)\n            S_pre.append(tmp_s)\n\n        if(verbose):\n            print('len T_pre', len(T_pre))\n            print('len T_pre[1]', len(T_pre[1]))\n\n        S_t_previ = torch.zeros([batch_size, in_channel, height, width]).to(self.configs.device)\n            \n        for t in range(self.configs.total_length - 1):\n            net = frames[:, t]\n            frames_feature = net\n            frames_feature_encoded = []\n            \n            if(verbose):\n                print('Len Encoders', len(self.encoders))\n                \n            for i in range(len(self.encoders)):\n                frames_feature = self.encoders[i](frames_feature)\n                if(verbose):\n                    print('Frames_feature', i, frames_feature.shape)\n                frames_feature_encoded.append(frames_feature)\n            if t == 0:\n                for i in range(self.num_layers):\n                    zeros = torch.zeros([batch_size, self.num_hidden[i], height, width]).to(self.configs.device)\n                    T_t.append(zeros)\n                    \n            S_t = frames_feature\n            if(verbose):\n                print('S_t in', S_t.shape)\n            \n            S_pixel_att = torch.sum((S_t - S_t_previ)**2)\n            S_t_previ = S_t\n            \n                \n            for i in range(self.num_layers):\n                t_att = T_pre[i][-self.tau:]\n                t_att = torch.stack(t_att, dim=0)\n                s_att = S_pre[i][-self.tau:]\n                s_att = torch.stack(s_att, dim=0)\n                S_pre[i].append(S_t)\n                if i < 2 and verbose:\n                    print('T_t', len(T_t))\n                    print('T_t[i], S_t, t_att, s_att', T_t[i].shape, S_t.shape, t_att.shape, s_att.shape)\n                T_t[i], S_t = self.cell_list[i](T_t[i], S_t, t_att, s_att, S_pixel_att)\n                T_pre[i].append(T_t[i])\n\n            next_frames.append(S_t)\n            next_memory.append(T_t[-1])\n            \n        next_frames = torch.stack(next_frames, dim=0).permute(1, 0, 2, 3, 4)\n        next_memory = torch.stack(next_memory, dim=0).permute(1, 0, 2, 3, 4)\n        next_all = torch.cat([next_frames, next_memory], dim=2)\n        return next_all","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.757971Z","iopub.execute_input":"2022-06-20T07:19:17.760322Z","iopub.status.idle":"2022-06-20T07:19:17.817767Z","shell.execute_reply.started":"2022-06-20T07:19:17.760277Z","shell.execute_reply":"2022-06-20T07:19:17.816894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN TEST WRAPPER**","metadata":{}},{"cell_type":"code","source":"def train_wrapper(model):\n    begin = 0\n\n    if args.pretrained_model_g:\n        model.load(args.pretrained_model_g, args.pretrained_model_d)\n        # begin = int(args.pretrained_model.split('-')[-1])\n        \n    # DATASET\n    dataset = TimeSeriesDataset(root_dir=configs.data_train_path, n_frames_input=10, n_frames_output=10)\n    \n    # DATA LOADER + SPLIT\n    validation_split = .3\n    shuffle_dataset = True\n    random_seed= 1000\n\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n    train_loader = torch.utils.data.DataLoader(dataset, args.batch_size, sampler=train_sampler, num_workers=2, pin_memory=True)\n    valid_loader = torch.utils.data.DataLoader(dataset, batch_size=4, sampler=valid_sampler)\n\n    losses_l1 = []\n    losses_l2 = []\n    \n    eta = args.sampling_start_value\n    eta -= (begin * args.sampling_changing_rate)\n    itr = begin\n\n#     print('Validate:')\n    evaluation_proper(model, valid_loader, args, args.output_length)\n    #return\n    \n    for epoch in range(0, args.max_epoches):\n        for x in tqdm(train_loader, total=len(train_loader)):\n            batch_size = x.shape[0]  # (bs, frames, c, h, w)\n            eta, mask = schedule_sampling(eta, itr, args.img_channel, batch_size)\n                \n            _, loss_l1, loss_l2 = model.train(x, mask, itr, next(iter(valid_loader)), epoch)\n            \n            if itr % configs.display_interval == 0:\n                print('Step: ' + str(itr), 'T L1 loss: ' + str(loss_l1), 'T L2 loss: ' + str(loss_l2))\n                \n            losses_l1.append(loss_l1)\n            losses_l2.append(loss_l2)\n            \n            if itr % configs.plot_interval == 0:\n                fig, ax = plt.subplots(2, 1, figsize=(13, 5))\n                a = ax.flatten()\n                a[0].plot(losses_l1, 'r')\n                a[0].set_title('Loss L1 (D)')\n                a[1].plot(losses_l2, 'r')\n                a[1].set_title('Loss L2 (G)')\n                plt.show()\n            \n            if itr % args.snapshot_interval == 0 and itr > begin:\n                model.save(itr)\n            itr += 1\n            \n        if epoch >=8:\n            print('Validate:')\n            evaluation_proper(model, valid_loader, args, args.output_length)\n            #model.save(itr)\n\n\ndef test_wrapper(model, valid_loader):\n    model.load(args.pretrained_model)\n\n    for i in range(1):\n        trainer.test(model, valid_loader, args, itr)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.828868Z","iopub.execute_input":"2022-06-20T07:19:17.832354Z","iopub.status.idle":"2022-06-20T07:19:17.860572Z","shell.execute_reply.started":"2022-06-20T07:19:17.832307Z","shell.execute_reply":"2022-06-20T07:19:17.859661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL FACTORY**","metadata":{}},{"cell_type":"code","source":"class Model(object):\n    def __init__(self, configs):\n        self.configs = configs\n        self.patch_height = configs.img_height\n        self.patch_width = configs.img_width\n        self.patch_channel = configs.img_channel\n        self.num_layers = configs.num_layers\n        networks_map = {'mau': RNN}\n        self.num_hidden = [configs.num_hidden for i in range(configs.num_layers)]\n        if configs.model_name in networks_map:\n            Network = networks_map[configs.model_name]\n            self.network = Network(self.num_layers, self.num_hidden, configs).to(configs.device)\n        else:\n            raise ValueError('Name of network unknown %s' % configs.model_name)\n\n        if args.gan:\n            self.disc = FDU(self.num_layers, self.num_hidden, configs).to(configs.device)\n            self.disc_optimizer = torch.optim.Adam(self.disc.parameters(), lr=configs.lr) #, betas=(0.5, 0.999))\n            #self.alpha = torch.linspace(1, 0, configs.max_epoches).to(configs.device)\n            #self.beta = torch.linspace(0, 1, configs.max_epoches).to(configs.device)\n            self.alpha = [1,1,1, 10, 100, 100, 100]\n            self.beta = [1,1,1, 0.1, 0.001, 0.001, 0.001]\n            \n        self.optimizer = Adam(self.network.parameters(), lr=configs.lr)\n        self.MSE_criterion = nn.MSELoss()\n        self.L1_loss = nn.L1Loss()\n        self.loss_bce = nn.BCEWithLogitsLoss()\n\n    def save(self, itr):\n        stats_g = {'net_param': self.network.state_dict()}\n        stats_d = {'net_param': self.disc.state_dict()}\n        checkpoint_path_g = os.path.join(self.configs.save_dir, 'model_g.ckpt' + '-' + str(itr))\n        checkpoint_path_d = os.path.join(self.configs.save_dir, 'model_d.ckpt' + '-' + str(itr))\n        torch.save(stats_g, checkpoint_path_g)\n        torch.save(stats_d, checkpoint_path_d)\n        #print(\"Save predictive model to %s\" % checkpoint_path)\n\n    def load(self, pm_checkpoint_path_g, pm_checkpoint_path_d):\n        print('Load predictive model:', pm_checkpoint_path_g)\n        stats = torch.load(pm_checkpoint_path_g, map_location=torch.device(self.configs.device))\n        self.network.load_state_dict(stats['net_param'])\n        stats = torch.load(pm_checkpoint_path_d, map_location=torch.device(self.configs.device))\n        self.disc.load_state_dict(stats['net_param'])\n\n    def train(self, data, mask, itr, val, ei):\n        frames = data\n        loss_d = 0\n        loss_g = 0\n        self.network.train()\n        val_tensor = torch.FloatTensor(val).to(self.configs.device)\n        frames_tensor = torch.FloatTensor(frames).to(self.configs.device)\n        mask_tensor = torch.FloatTensor(mask).to(self.configs.device)\n\n        if(self.configs.verbose):\n            print('FT', frames_tensor.shape)\n            print('MT', mask_tensor.shape)\n\n        next_frames = self.network(frames_tensor, mask_tensor)\n        if(self.configs.verbose):\n            print('Next Frames', next_frames.shape)\n\n        ground_truth = frames_tensor\n        if(self.configs.verbose):\n            print('Ground', ground_truth[:, 1:].shape)\n            \n        if args.gan:    \n            d_fake = self.disc(next_frames.detach()) #decoder_input.detach(), \n            d_real = self.disc(frames_tensor[:, 1:]) #decoder_input.detach(), \n            loss_d_real = self.loss_bce(d_real, torch.ones_like(d_real))\n            loss_d_fake = self.loss_bce(d_fake, torch.zeros_like(d_fake))\n            loss_d = loss_d_real + loss_d_fake\n\n            \n        if itr % configs.plot_interval == 0:\n            print('Epoch:', ei)\n            with torch.no_grad():\n                self.network.eval()\n                x = frames_tensor[0][0:configs.input_length]\n                y = frames_tensor[0][configs.input_length:]\n                g = next_frames[0][configs.input_length-1:]\n                m = mask_tensor[0]\n                fig, ax = plt.subplots(4, configs.input_length, figsize=(25, 10))\n                for i in range(4):\n                    for j in range(configs.input_length):\n                        if i == 0:\n                            ax[i][j].imshow(x[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('T Input')\n                        if i == 1:\n                            if j == configs.input_length-1:\n                                ax[i][j].axis('off')\n                                continue\n                            ax[i][j].imshow(m[j].to('cpu'))\n                            ax[i][j].set_title('T Mask')\n                        if i == 2:\n                            ax[i][j].imshow(y[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('T Ground Truth')\n                        if i == 3:\n                            ax[i][j].imshow(g[j].detach().to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('T Generated')\n                        ax[i][j].axis('off')\n\n\n                x = val_tensor[0][0:configs.input_length]\n                y = val_tensor[0][configs.input_length:]\n                mask = torch.zeros_like(mask_tensor[0]).unsqueeze(0).to(configs.device)\n                next_frameszz = self.network(val_tensor, mask)\n                m = mask[0]\n                g = next_frameszz[0][configs.input_length-1:]\n                fig, ax = plt.subplots(4, configs.input_length, figsize=(25, 10))\n                for i in range(4):\n                    for j in range(configs.input_length):\n                        if i == 0:\n                            ax[i][j].imshow(x[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('V Input')\n                        if i == 1:\n                            if j == configs.input_length-1:\n                                ax[i][j].axis('off')\n                                continue\n                            ax[i][j].imshow(m[j].to('cpu'))\n                            ax[i][j].set_title('V Mask')\n                        if i == 2:\n                            ax[i][j].imshow(y[j].to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('V Ground Truth')\n                        if i == 3:\n                            ax[i][j].imshow(g[j].detach().to('cpu').permute(1, 2, 0), cmap='gray')\n                            ax[i][j].set_title('V Generated')\n                        ax[i][j].axis('off')\n                \n                self.network.train()\n        \n        if args.gan:\n            self.disc_optimizer.zero_grad()\n            loss_d.backward()\n            self.disc_optimizer.step() \n\n            \n            d_fake = self.disc(next_frames) \n            loss_g = self.loss_bce(d_fake, torch.ones_like(d_fake))\n            \n        batch_size = next_frames.shape[0]\n\n        loss_l1 = self.L1_loss(next_frames, ground_truth[:, 1:])\n        loss_l2 = self.MSE_criterion(next_frames, ground_truth[:, 1:])\n        \n        alp = self.alpha[min(len(self.alpha)-1, ei)]\n        bet = self.beta[min(len(self.beta)-1, ei)]\n        \n        if itr % configs.plot_interval == 0:\n            print('Aplha: ', alp)\n            print('Beta: ', bet)\n        \n        if args.gan:\n            #loss_gen = loss_l2*alp + loss_l1 + loss_g*bet\n            loss_gen = loss_g*1e-6 + loss_l2 + loss_l1*0.1\n            #loss_gen = loss_g*1e-6*alp + loss_l2*bet + loss_l1*0.5\n            #loss_gen = loss_g*1e-6*alp + loss_l2*bet + loss_l1*0.5\n        else:\n            loss_gen = loss_l2 # + loss_l1*0.5\n        \n        self.optimizer.zero_grad()\n        loss_gen.backward()\n        self.optimizer.step()\n\n        if itr >= self.configs.sampling_stop_iter and itr % self.configs.delay_interval == 0:\n            self.scheduler.step()\n            print('LR decay to:%.8f', self.optimizer.param_groups[0]['lr'])\n            \n        return next_frames, loss_d.item(), loss_gen.item()\n\n    def test(self, data, mask):\n        frames = data\n        self.network.eval()\n        frames_tensor = torch.FloatTensor(frames).to(self.configs.device)\n        mask_tensor = torch.FloatTensor(mask).to(self.configs.device)\n        next_frames = self.network(frames_tensor, mask_tensor)\n        return next_frames.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.870132Z","iopub.execute_input":"2022-06-20T07:19:17.872579Z","iopub.status.idle":"2022-06-20T07:19:17.955116Z","shell.execute_reply.started":"2022-06-20T07:19:17.872533Z","shell.execute_reply":"2022-06-20T07:19:17.953391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CONFIG**","metadata":{}},{"cell_type":"code","source":"class Configuration:\n    def __init__(self):\n        super(Configuration, self).__init__()\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.data_train_path = '../input/casiabpretreated/Here'\n        self.data_test_path = '../input/kthextract-to-jpg/data/test'\n        self.input_length = 10\n        self.output_length = 10\n        self.real_length = 20\n        self.total_length = 20\n        self.img_height = 64\n        self.img_width = 64\n        self.sr_size = 4\n        self.img_channel = 1\n        self.patch_size = 1\n        self.alpha = 1\n        self.model_name = 'mau'\n        self.dataset = 'custom'\n        self.cell_mode = 'normal'\n        self.model_mode = 'recall'\n        self.num_workers = 2\n        self.num_hidden = 64\n        self.num_layers = 2\n        self.num_heads = 2\n        self.filter_size = (5, 5)\n        self.stride = 1\n        self.time = 2\n        self.time_stride = 1\n        self.tau = 7\n        self.is_training = True\n        self.lr = 2e-4\n        self.lr_decay = 0.90\n        self.delay_interval = 2000\n        self.batch_size = 16\n        self.max_iterations = 150000\n        self.max_epoches = 10\n        self.display_interval = 75\n        self.plot_interval = 75\n        self.test_interval = 1010\n        self.snapshot_interval = 1000\n        self.num_save_samples = 3\n        self.n_gpu = 1\n        self.pretrained_model_g = '../input/afterganwts/model_g.ckpt-7500'\n        self.pretrained_model_d = '../input/afterganwts/model_d.ckpt-7500'\n        self.perforamnce_dir = 'results/custom'\n        self.save_dir = 'saves/custom'\n        self.gen_frm_dir = 'results/custom/'\n        self.scheduled_sampling = True\n        self.sampling_stop_iter = 50000\n        self.sampling_start_value = 0.80\n        self.sampling_changing_rate = 0.00007\n        self.gan = True\n        self.verbose = False\n        \nconfigs = Configuration()\nargs = configs","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:17.966307Z","iopub.execute_input":"2022-06-20T07:19:17.966607Z","iopub.status.idle":"2022-06-20T07:19:18.001009Z","shell.execute_reply.started":"2022-06-20T07:19:17.966569Z","shell.execute_reply":"2022-06-20T07:19:17.999266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST CASE","metadata":{}},{"cell_type":"code","source":"def test_model(configs):\n    nl = 4\n    nh = [64, 64, 64, 64]\n    z = torch.randn(1, 20, 3, 200, 160).to(configs.device)\n    m = torch.zeros(1, 9, 200, 160, 3).to(configs.device)\n    model = RNN(nl, nh, configs).to(configs.device)\n    g = model(z, m, True)\n    print(g.shape)\n\n# test_model(configs)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:18.002847Z","iopub.execute_input":"2022-06-20T07:19:18.003108Z","iopub.status.idle":"2022-06-20T07:19:18.0232Z","shell.execute_reply.started":"2022-06-20T07:19:18.003073Z","shell.execute_reply":"2022-06-20T07:19:18.020086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRAIN**","metadata":{}},{"cell_type":"code","source":"print('Initializing models')\n\nmodel = Model(args)\n\nif args.is_training:\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n    if not os.path.exists(args.gen_frm_dir):\n        os.makedirs(args.gen_frm_dir)\n    train_wrapper(model)\nelse:\n    if not os.path.exists(args.gen_frm_dir):\n        os.makedirs(args.gen_frm_dir)\n    test_wrapper(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:19:18.024599Z","iopub.execute_input":"2022-06-20T07:19:18.025019Z"},"trusted":true},"execution_count":null,"outputs":[]}]}